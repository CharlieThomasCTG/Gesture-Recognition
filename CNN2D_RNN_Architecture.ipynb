{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gesture Recognition\n",
    "In this group project, you are going to build a 2D CNN + RNN model that will be able to predict the 5 gestures correctly. Please import the following libraries to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "#we will use cv2 here\n",
    "import cv2\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.applications.resnet import ResNet50\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, LSTM, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation, Dropout, GlobalAveragePooling2D\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D, Conv2D, MaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the random seed so that the results don't vary drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "663 100\n"
     ]
    }
   ],
   "source": [
    "train_doc = np.random.permutation(open('train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('val.csv').readlines())\n",
    "print(len(train_doc), len(val_doc))\n",
    "batch_size = 50 #experiment with the batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "This is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with `img_idx`, `y`,`z` and normalization such that you get high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_f = 70"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have modified the generator function by adding 2 more parameters scale_f -> size of image to be rescaled and n_i -> number of images to train on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(source_path, folder_list, batch_size, scale_f, n_i):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    #create a list of image numbers you want to use for a particular video\n",
    "    img_idx = [i for i in range(30)]\n",
    "    if(n_i == 20):\n",
    "        img_idx = [0,1,4,5,7,8,10,11,13,14,16,17,19,20,22,23,25,26,28,29]\n",
    "    elif(n_i == 15):\n",
    "        img_idx = [i for i in range(0,30,2)]\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        # calculate the number of batches\n",
    "        num_batches = int(len(folder_list)/batch_size)\n",
    "    \n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,len(img_idx),scale_f,scale_f,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = cv2.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    \n",
    "                    #We have 160*120 and 360*360 images, we will resize every image to scale_f*scale_f\n",
    "                    image = cv2.resize(image, (scale_f, scale_f))\n",
    "                    \n",
    "                    \n",
    "                    #normalising using min-max method\n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0] - np.min(image[:,:,0])/np.max(image[:,:,0]) - np.min(image[:,:,0])#normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1] - np.min(image[:,:,1])/np.max(image[:,:,1]) - np.min(image[:,:,1])#normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2] - np.min(image[:,:,2])/np.max(image[:,:,2]) - np.min(image[:,:,2])#normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        \n",
    "        left_samples = len(folder_list)%batch_size\n",
    "        offset = len(folder_list) - left_samples\n",
    "        if (left_samples > 0):\n",
    "            batch_data = np.zeros((left_samples,len(img_idx),scale_f,scale_f,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((left_samples,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(left_samples): # iterate over the left_samples\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + offset].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = cv2.imread(source_path+'/'+ t[folder + offset].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "\n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "\n",
    "                    #We have 160*120 and 360*360 images, we will resize every image to scale_f*scale_f\n",
    "                    image = cv2.resize(image, (scale_f, scale_f))\n",
    "\n",
    "\n",
    "                    #normalising using min-max method\n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0] - np.min(image[:,:,0])/np.max(image[:,:,0]) - np.min(image[:,:,0])#normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1] - np.min(image[:,:,1])/np.max(image[:,:,1]) - np.min(image[:,:,1])#normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2] - np.min(image[:,:,2])/np.max(image[:,:,2]) - np.min(image[:,:,2])#normalise and feed in the image\n",
    "\n",
    "                batch_labels[folder, int(t[folder + offset].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here that a video is represented above in the generator as (number of images, height, width, number of channels). Take this into consideration while creating the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 1\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = 'train'\n",
    "val_path = 'val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 1# choose the number of epochs\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1\n",
    "Here you make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. You would want to use `TimeDistributed` while building a Conv2D + RNN model. Also remember that the last layer is the softmax. Design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(30)\n",
    "#initially building a simple model to test whether model is fitting and generator is running or not\n",
    "model = Sequential()\n",
    "n_i = 30\n",
    "model.add(TimeDistributed(Conv2D(64,(3,3),activation='relu',input_shape=(n_i, scale_f, scale_f, 3))))#(inputs))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "model.add(TimeDistributed(Dropout(0.25)))\n",
    "\n",
    "# flatten\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "#addig GRU layers\n",
    "model.add(GRU(64))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed (TimeDistr  (None, 30, 68, 68, 64)   1792      \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDis  (None, 30, 34, 34, 64)   0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_2 (TimeDis  (None, 30, 34, 34, 64)   0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_3 (TimeDis  (None, 30, 73984)        0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 64)                14217600  \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               6500      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 505       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,226,397\n",
      "Trainable params: 14,226,397\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser = 'sgd'#optimizer\n",
    "\n",
    "#building model with input\n",
    "input_param = (None, n_i, scale_f,scale_f,3)\n",
    "model.build(input_param)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size, scale_f, n_i)\n",
    "val_generator = generator(val_path, val_doc, batch_size, scale_f, n_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','--').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=1, min_lr=0.001) # Reducelronplateau code\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and Fit the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  train ; batch size = 50\n",
      "Epoch 1/10\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.7681 - categorical_accuracy: 0.2112Source path =  val ; batch size = 50\n",
      "\n",
      "Epoch 1: saving model to model_init_2022-05-16--17_28_33.805466\\model-00001-1.76814-0.21116-1.65975-0.16000.h5\n",
      "14/14 [==============================] - 79s 5s/step - loss: 1.7681 - categorical_accuracy: 0.2112 - val_loss: 1.6597 - val_categorical_accuracy: 0.1600 - lr: 0.0100\n",
      "Epoch 2/10\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.6820 - categorical_accuracy: 0.1885\n",
      "Epoch 2: saving model to model_init_2022-05-16--17_28_33.805466\\model-00002-1.68203-0.18854-1.65610-0.14000.h5\n",
      "14/14 [==============================] - 80s 6s/step - loss: 1.6820 - categorical_accuracy: 0.1885 - val_loss: 1.6561 - val_categorical_accuracy: 0.1400 - lr: 0.0100\n",
      "Epoch 3/10\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.6529 - categorical_accuracy: 0.1900\n",
      "Epoch 3: saving model to model_init_2022-05-16--17_28_33.805466\\model-00003-1.65289-0.19005-1.61355-0.16000.h5\n",
      "14/14 [==============================] - 81s 6s/step - loss: 1.6529 - categorical_accuracy: 0.1900 - val_loss: 1.6136 - val_categorical_accuracy: 0.1600 - lr: 0.0100\n",
      "Epoch 4/10\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.6474 - categorical_accuracy: 0.1885\n",
      "Epoch 4: saving model to model_init_2022-05-16--17_28_33.805466\\model-00004-1.64738-0.18854-1.60495-0.21000.h5\n",
      "14/14 [==============================] - 79s 6s/step - loss: 1.6474 - categorical_accuracy: 0.1885 - val_loss: 1.6050 - val_categorical_accuracy: 0.2100 - lr: 0.0100\n",
      "Epoch 5/10\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.6485 - categorical_accuracy: 0.2006\n",
      "Epoch 5: saving model to model_init_2022-05-16--17_28_33.805466\\model-00005-1.64851-0.20060-1.61459-0.21000.h5\n",
      "14/14 [==============================] - 82s 6s/step - loss: 1.6485 - categorical_accuracy: 0.2006 - val_loss: 1.6146 - val_categorical_accuracy: 0.2100 - lr: 0.0100\n",
      "Epoch 6/10\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.6374 - categorical_accuracy: 0.2157\n",
      "Epoch 6: saving model to model_init_2022-05-16--17_28_33.805466\\model-00006-1.63736-0.21569-1.61393-0.21000.h5\n",
      "14/14 [==============================] - 79s 6s/step - loss: 1.6374 - categorical_accuracy: 0.2157 - val_loss: 1.6139 - val_categorical_accuracy: 0.2100 - lr: 0.0020\n",
      "Epoch 7/10\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.6457 - categorical_accuracy: 0.1855\n",
      "Epoch 7: saving model to model_init_2022-05-16--17_28_33.805466\\model-00007-1.64570-0.18552-1.62370-0.20000.h5\n",
      "14/14 [==============================] - 75s 5s/step - loss: 1.6457 - categorical_accuracy: 0.1855 - val_loss: 1.6237 - val_categorical_accuracy: 0.2000 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.6507 - categorical_accuracy: 0.2081\n",
      "Epoch 8: saving model to model_init_2022-05-16--17_28_33.805466\\model-00008-1.65070-0.20814-1.62247-0.18000.h5\n",
      "14/14 [==============================] - 76s 6s/step - loss: 1.6507 - categorical_accuracy: 0.2081 - val_loss: 1.6225 - val_categorical_accuracy: 0.1800 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.6505 - categorical_accuracy: 0.2021\n",
      "Epoch 9: saving model to model_init_2022-05-16--17_28_33.805466\\model-00009-1.65053-0.20211-1.60997-0.21000.h5\n",
      "14/14 [==============================] - 73s 5s/step - loss: 1.6505 - categorical_accuracy: 0.2021 - val_loss: 1.6100 - val_categorical_accuracy: 0.2100 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.6495 - categorical_accuracy: 0.1825\n",
      "Epoch 10: saving model to model_init_2022-05-16--17_28_33.805466\\model-00010-1.64953-0.18250-1.61191-0.20000.h5\n",
      "14/14 [==============================] - 74s 5s/step - loss: 1.6495 - categorical_accuracy: 0.1825 - val_loss: 1.6119 - val_categorical_accuracy: 0.2000 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "history=model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=10, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findings: Model 1\n",
    "* Model has very low accuracy.\n",
    "* We will create new model with additional conv2d timedistributed layers\n",
    "- Train loss: 1.6495 \n",
    "- Train categorical_accuracy: 0.1825 \n",
    "- val_loss: 1.6119 \n",
    "- val_categorical_accuracy: 0.2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2\n",
    "* We have included additional conv2d timedistributed layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed_4 (TimeDis  (None, 30, 68, 68, 64)   1792      \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_5 (TimeDis  (None, 30, 68, 68, 64)   0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_6 (TimeDis  (None, 30, 68, 68, 64)   256       \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_7 (TimeDis  (None, 30, 66, 66, 32)   18464     \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_8 (TimeDis  (None, 30, 66, 66, 32)   0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_9 (TimeDis  (None, 30, 66, 66, 32)   128       \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_10 (TimeDi  (None, 30, 33, 33, 32)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_11 (TimeDi  (None, 30, 33, 33, 32)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_12 (TimeDi  (None, 30, 33, 33, 64)   18496     \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_13 (TimeDi  (None, 30, 33, 33, 64)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_14 (TimeDi  (None, 30, 33, 33, 64)   256       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_15 (TimeDi  (None, 30, 31, 31, 64)   36928     \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_16 (TimeDi  (None, 30, 31, 31, 64)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_17 (TimeDi  (None, 30, 31, 31, 64)   256       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_18 (TimeDi  (None, 30, 15, 15, 64)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_19 (TimeDi  (None, 30, 15, 15, 64)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_20 (TimeDi  (None, 30, 14400)        0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 30, 64)            2777472   \n",
      "                                                                 \n",
      " time_distributed_21 (TimeDi  (None, 30, 64)           0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 1920)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 100)               192100    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 5)                 505       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,046,653\n",
      "Trainable params: 3,046,205\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#since Last model is very bad in terms of accuracy, we will add more conv2d timedistributed layers \n",
    "\n",
    "model = Sequential()\n",
    "n_i = 30\n",
    "model.add(TimeDistributed(Conv2D(64,(3,3),activation='relu',input_shape=(n_i, scale_f, scale_f, 3))))\n",
    "model.add(TimeDistributed(Activation('relu')))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(32, (3, 3))))\n",
    "model.add(TimeDistributed(Activation('relu')))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "model.add(TimeDistributed(Dropout(0.25)))\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(64, (3, 3), padding='same')))\n",
    "model.add(TimeDistributed(Activation('relu')))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(Conv2D(64, (3, 3))))\n",
    "model.add(TimeDistributed((Activation('relu'))))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "model.add(TimeDistributed(Dropout(0.25)))\n",
    "\n",
    "\n",
    "# flatten\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "#adding GRU layer with dropout\n",
    "model.add(GRU(64,return_sequences=True))\n",
    "model.add(TimeDistributed(Dropout(0.25)))\n",
    "\n",
    "#flattening again for dense input\n",
    "model.add(Flatten())\n",
    "\n",
    "#taking a single dense before output softmax\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "optimiser = 'sgd'#optimizer\n",
    "\n",
    "input_param = (None, n_i, scale_f,scale_f,3)\n",
    "model.build(input_param)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "#changing batch size to fit in memory\n",
    "batch_size = 30\n",
    "\n",
    "#do for 10 epochs\n",
    "num_epochs = 10\n",
    "train_generator = generator(train_path, train_doc, batch_size,scale_f,n_i)\n",
    "val_generator = generator(val_path, val_doc, batch_size,scale_f,n_i)\n",
    "\n",
    "#save model weights\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','--').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=5, min_lr=0.001) # Reducelronplateau code\n",
    "callbacks_list = [checkpoint, LR]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#steps per epoch\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and Fit the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\44775\\AppData\\Local\\Temp\\ipykernel_13840\\3178268726.py:3: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  train ; batch size = 30\n",
      "Epoch 1/10\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.5230 - categorical_accuracy: 0.3605Source path =  val ; batch size = 30\n",
      "\n",
      "Epoch 1: saving model to model_init_2022-05-16--17_28_33.805466\\model-00001-1.52304-0.36048-2.63750-0.18000.h5\n",
      "23/23 [==============================] - 213s 9s/step - loss: 1.5230 - categorical_accuracy: 0.3605 - val_loss: 2.6375 - val_categorical_accuracy: 0.1800 - lr: 0.0100\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.2686 - categorical_accuracy: 0.5189\n",
      "Epoch 2: saving model to model_init_2022-05-16--17_28_33.805466\\model-00002-1.26863-0.51885-4.27341-0.26000.h5\n",
      "23/23 [==============================] - 207s 9s/step - loss: 1.2686 - categorical_accuracy: 0.5189 - val_loss: 4.2734 - val_categorical_accuracy: 0.2600 - lr: 0.0100\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.9509 - categorical_accuracy: 0.6244\n",
      "Epoch 3: saving model to model_init_2022-05-16--17_28_33.805466\\model-00003-0.95087-0.62443-3.66814-0.20000.h5\n",
      "23/23 [==============================] - 210s 9s/step - loss: 0.9509 - categorical_accuracy: 0.6244 - val_loss: 3.6681 - val_categorical_accuracy: 0.2000 - lr: 0.0100\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.5803 - categorical_accuracy: 0.7783\n",
      "Epoch 4: saving model to model_init_2022-05-16--17_28_33.805466\\model-00004-0.58031-0.77828-4.94279-0.22000.h5\n",
      "23/23 [==============================] - 206s 9s/step - loss: 0.5803 - categorical_accuracy: 0.7783 - val_loss: 4.9428 - val_categorical_accuracy: 0.2200 - lr: 0.0100\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.5014 - categorical_accuracy: 0.8145\n",
      "Epoch 5: saving model to model_init_2022-05-16--17_28_33.805466\\model-00005-0.50139-0.81448-2.35999-0.34000.h5\n",
      "23/23 [==============================] - 206s 9s/step - loss: 0.5014 - categorical_accuracy: 0.8145 - val_loss: 2.3600 - val_categorical_accuracy: 0.3400 - lr: 0.0100\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.3741 - categorical_accuracy: 0.8703\n",
      "Epoch 6: saving model to model_init_2022-05-16--17_28_33.805466\\model-00006-0.37407-0.87029-5.21551-0.23000.h5\n",
      "23/23 [==============================] - 211s 9s/step - loss: 0.3741 - categorical_accuracy: 0.8703 - val_loss: 5.2155 - val_categorical_accuracy: 0.2300 - lr: 0.0100\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.3340 - categorical_accuracy: 0.8869\n",
      "Epoch 7: saving model to model_init_2022-05-16--17_28_33.805466\\model-00007-0.33402-0.88688-1.74093-0.49000.h5\n",
      "23/23 [==============================] - 209s 9s/step - loss: 0.3340 - categorical_accuracy: 0.8869 - val_loss: 1.7409 - val_categorical_accuracy: 0.4900 - lr: 0.0100\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.2171 - categorical_accuracy: 0.9276\n",
      "Epoch 8: saving model to model_init_2022-05-16--17_28_33.805466\\model-00008-0.21708-0.92760-2.25046-0.47000.h5\n",
      "23/23 [==============================] - 206s 9s/step - loss: 0.2171 - categorical_accuracy: 0.9276 - val_loss: 2.2505 - val_categorical_accuracy: 0.4700 - lr: 0.0100\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1872 - categorical_accuracy: 0.9397\n",
      "Epoch 9: saving model to model_init_2022-05-16--17_28_33.805466\\model-00009-0.18723-0.93967-2.60366-0.46000.h5\n",
      "23/23 [==============================] - 209s 9s/step - loss: 0.1872 - categorical_accuracy: 0.9397 - val_loss: 2.6037 - val_categorical_accuracy: 0.4600 - lr: 0.0100\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.2903 - categorical_accuracy: 0.9020\n",
      "Epoch 10: saving model to model_init_2022-05-16--17_28_33.805466\\model-00010-0.29035-0.90196-0.68281-0.75000.h5\n",
      "23/23 [==============================] - 214s 9s/step - loss: 0.2903 - categorical_accuracy: 0.9020 - val_loss: 0.6828 - val_categorical_accuracy: 0.7500 - lr: 0.0100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1beb7fbfca0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    \n",
    "#fit model\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findings: Model 2\n",
    "\n",
    "* Model 2 is overfitting. Accuracy on train dataset is 0.9020 where as Accuracy on validation dataset is 0.7500\n",
    "* We will experiment by increasing the size of scaled images and check if the new model can manage the issue of overfitting.\n",
    "* \n",
    "  Total params: 3,046,653\n",
    "  Trainable params: 3,046,205\n",
    "  Non-trainable params: 448\n",
    "  \n",
    "- Train loss: 0.2903 \n",
    "- Train categorical_accuracy: 0.9020 \n",
    "- val_loss: 0.6828 \n",
    "- val_categorical_accuracy: 0.7500 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3\n",
    "* Creating a model with size of the scaled image =70\n",
    "* adding another set of conv2D timedistributed layer for better feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed_22 (TimeDi  (None, 30, 68, 68, 64)   1792      \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_23 (TimeDi  (None, 30, 68, 68, 64)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_24 (TimeDi  (None, 30, 68, 68, 64)   256       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_25 (TimeDi  (None, 30, 66, 66, 32)   18464     \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_26 (TimeDi  (None, 30, 66, 66, 32)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_27 (TimeDi  (None, 30, 66, 66, 32)   128       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_28 (TimeDi  (None, 30, 33, 33, 32)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_29 (TimeDi  (None, 30, 33, 33, 32)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_30 (TimeDi  (None, 30, 33, 33, 64)   18496     \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_31 (TimeDi  (None, 30, 33, 33, 64)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_32 (TimeDi  (None, 30, 33, 33, 64)   256       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_33 (TimeDi  (None, 30, 31, 31, 64)   36928     \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_34 (TimeDi  (None, 30, 31, 31, 64)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_35 (TimeDi  (None, 30, 31, 31, 64)   256       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_36 (TimeDi  (None, 30, 15, 15, 64)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_37 (TimeDi  (None, 30, 15, 15, 64)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_38 (TimeDi  (None, 30, 15, 15, 128)  73856     \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_39 (TimeDi  (None, 30, 15, 15, 128)  0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_40 (TimeDi  (None, 30, 15, 15, 128)  512       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_41 (TimeDi  (None, 30, 13, 13, 128)  147584    \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_42 (TimeDi  (None, 30, 13, 13, 128)  0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_43 (TimeDi  (None, 30, 13, 13, 128)  512       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_44 (TimeDi  (None, 30, 6, 6, 128)    0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_45 (TimeDi  (None, 30, 6, 6, 128)    0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_46 (TimeDi  (None, 30, 4608)         0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " gru_2 (GRU)                 (None, 30, 64)            897408    \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 1920)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 512)               983552    \n",
      "                                                                 \n",
      " activation_10 (Activation)  (None, 512)               0         \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 5)                 2565      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,182,565\n",
      "Trainable params: 2,181,605\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#size of scaled image =70\n",
    "scale_f = 70\n",
    "\n",
    "model = Sequential()\n",
    "n_i = 30\n",
    "model.add(TimeDistributed(Conv2D(64,(3,3),activation='relu',input_shape=(n_i, scale_f, scale_f, 3))))\n",
    "model.add(TimeDistributed(Activation('relu')))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(32, (3, 3))))\n",
    "model.add(TimeDistributed(Activation('relu')))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "model.add(TimeDistributed(Dropout(0.25)))\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(64, (3, 3), padding='same')))\n",
    "model.add(TimeDistributed(Activation('relu')))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(Conv2D(64, (3, 3))))\n",
    "model.add(TimeDistributed((Activation('relu'))))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "model.add(TimeDistributed(Dropout(0.25)))\n",
    "\n",
    "\n",
    "#we have added another set of conv2d timedistributed layers for better feature extraction\n",
    "model.add(TimeDistributed(Conv2D(128, (3, 3), padding='same')))\n",
    "model.add(TimeDistributed(Activation('relu')))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(Conv2D(128, (3, 3))))\n",
    "model.add(TimeDistributed(Activation('relu')))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "model.add(TimeDistributed(Dropout(0.25)))\n",
    "\n",
    "# flatten\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "#adding GRU layers\n",
    "model.add(GRU(64,return_sequences=True))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "#increasing the dense layer size to 512 for better learning\n",
    "model.add(Dense(512,kernel_regularizer=l2(0.01)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "optimiser = 'sgd'#optimizer\n",
    "\n",
    "input_param = (None, n_i, scale_f,scale_f,3)\n",
    "model.build(input_param)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and Fit the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Source path =  train ; batch size = 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\44775\\AppData\\Local\\Temp\\ipykernel_13840\\1776157345.py:34: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "34/34 [==============================] - ETA: 0s - loss: 9.4205 - categorical_accuracy: 0.4072Source path =  val ; batch size = 20\n",
      "\n",
      "Epoch 1: saving model to model_init_2022-05-16--17_28_33.805466\\model-00001-9.42045-0.40724-12.43602-0.23000.h5\n",
      "34/34 [==============================] - 262s 8s/step - loss: 9.4205 - categorical_accuracy: 0.4072 - val_loss: 12.4360 - val_categorical_accuracy: 0.2300 - lr: 0.0100\n",
      "Epoch 2/10\n",
      "34/34 [==============================] - ETA: 0s - loss: 9.0109 - categorical_accuracy: 0.5656\n",
      "Epoch 2: saving model to model_init_2022-05-16--17_28_33.805466\\model-00002-9.01089-0.56561-11.42424-0.23000.h5\n",
      "34/34 [==============================] - 229s 7s/step - loss: 9.0109 - categorical_accuracy: 0.5656 - val_loss: 11.4242 - val_categorical_accuracy: 0.2300 - lr: 0.0100\n",
      "Epoch 3/10\n",
      "34/34 [==============================] - ETA: 0s - loss: 8.5514 - categorical_accuracy: 0.7436\n",
      "Epoch 3: saving model to model_init_2022-05-16--17_28_33.805466\\model-00003-8.55141-0.74359-9.66569-0.38000.h5\n",
      "34/34 [==============================] - 232s 7s/step - loss: 8.5514 - categorical_accuracy: 0.7436 - val_loss: 9.6657 - val_categorical_accuracy: 0.3800 - lr: 0.0100\n",
      "Epoch 4/10\n",
      "34/34 [==============================] - ETA: 0s - loss: 8.2989 - categorical_accuracy: 0.7843\n",
      "Epoch 4: saving model to model_init_2022-05-16--17_28_33.805466\\model-00004-8.29889-0.78431-8.70666-0.66000.h5\n",
      "34/34 [==============================] - 228s 7s/step - loss: 8.2989 - categorical_accuracy: 0.7843 - val_loss: 8.7067 - val_categorical_accuracy: 0.6600 - lr: 0.0100\n",
      "Epoch 5/10\n",
      "34/34 [==============================] - ETA: 0s - loss: 8.0518 - categorical_accuracy: 0.8341\n",
      "Epoch 5: saving model to model_init_2022-05-16--17_28_33.805466\\model-00005-8.05175-0.83409-8.14177-0.82000.h5\n",
      "34/34 [==============================] - 228s 7s/step - loss: 8.0518 - categorical_accuracy: 0.8341 - val_loss: 8.1418 - val_categorical_accuracy: 0.8200 - lr: 0.0100\n",
      "Epoch 6/10\n",
      "34/34 [==============================] - ETA: 0s - loss: 7.8593 - categorical_accuracy: 0.8793\n",
      "Epoch 6: saving model to model_init_2022-05-16--17_28_33.805466\\model-00006-7.85927-0.87934-8.17197-0.72000.h5\n",
      "34/34 [==============================] - 229s 7s/step - loss: 7.8593 - categorical_accuracy: 0.8793 - val_loss: 8.1720 - val_categorical_accuracy: 0.7200 - lr: 0.0100\n",
      "Epoch 7/10\n",
      "34/34 [==============================] - ETA: 0s - loss: 7.6679 - categorical_accuracy: 0.8914\n",
      "Epoch 7: saving model to model_init_2022-05-16--17_28_33.805466\\model-00007-7.66791-0.89140-7.87266-0.83000.h5\n",
      "34/34 [==============================] - 229s 7s/step - loss: 7.6679 - categorical_accuracy: 0.8914 - val_loss: 7.8727 - val_categorical_accuracy: 0.8300 - lr: 0.0100\n",
      "Epoch 8/10\n",
      "34/34 [==============================] - ETA: 0s - loss: 7.5093 - categorical_accuracy: 0.9397\n",
      "Epoch 8: saving model to model_init_2022-05-16--17_28_33.805466\\model-00008-7.50933-0.93967-7.98158-0.79000.h5\n",
      "34/34 [==============================] - 235s 7s/step - loss: 7.5093 - categorical_accuracy: 0.9397 - val_loss: 7.9816 - val_categorical_accuracy: 0.7900 - lr: 0.0100\n",
      "Epoch 9/10\n",
      "34/34 [==============================] - ETA: 0s - loss: 7.3416 - categorical_accuracy: 0.9683\n",
      "Epoch 9: saving model to model_init_2022-05-16--17_28_33.805466\\model-00009-7.34159-0.96833-8.63032-0.50000.h5\n",
      "34/34 [==============================] - 236s 7s/step - loss: 7.3416 - categorical_accuracy: 0.9683 - val_loss: 8.6303 - val_categorical_accuracy: 0.5000 - lr: 0.0100\n",
      "Epoch 10/10\n",
      "34/34 [==============================] - ETA: 0s - loss: 7.3055 - categorical_accuracy: 0.9367\n",
      "Epoch 10: saving model to model_init_2022-05-16--17_28_33.805466\\model-00010-7.30548-0.93665-7.75964-0.76000.h5\n",
      "34/34 [==============================] - 232s 7s/step - loss: 7.3055 - categorical_accuracy: 0.9367 - val_loss: 7.7596 - val_categorical_accuracy: 0.7600 - lr: 0.0100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1beea21a080>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 20\n",
    "num_epochs = 10\n",
    "train_generator = generator(train_path, train_doc, batch_size,scale_f, n_i)\n",
    "val_generator = generator(val_path, val_doc, batch_size,scale_f, n_i)\n",
    "\n",
    "#save model weights\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','--').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=5, min_lr=0.001) #Reducelronplateau code \n",
    "callbacks_list = [checkpoint, LR]\n",
    "\n",
    "\n",
    "#steps per epoch\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1\n",
    "    \n",
    "#fit model\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings\n",
    "* models in few epochs has manage to sort the overfitting issue but later epoch still shown overfitting. \n",
    "* we will try to reduce the overfitting as well as loss in next model\n",
    "\n",
    "- Train loss: 7.3055 \n",
    "- Train categorical_accuracy: 0.9367 \n",
    "- val_loss: 7.7596 \n",
    "- val_categorical_accuracy: 0.7600\n",
    "\n",
    "* Total params: 2,182,565\n",
    "  Trainable params: 2,181,605\n",
    "  Non-trainable params: 960\n",
    "___________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build, Compile and Fit the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Source path =  train ; batch size = 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\44775\\AppData\\Local\\Temp\\ipykernel_13840\\2170958243.py:35: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 7.1317 - categorical_accuracy: 0.9608Source path =  val ; batch size = 20\n",
      "\n",
      "Epoch 1: saving model to model_init_2022-05-16--17_28_33.805466\\model-00001-7.13168-0.96078-7.74347-0.76000.h5\n",
      "34/34 [==============================] - 233s 7s/step - loss: 7.1317 - categorical_accuracy: 0.9608 - val_loss: 7.7435 - val_categorical_accuracy: 0.7600 - lr: 0.0100\n",
      "Epoch 2/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 6.9974 - categorical_accuracy: 0.9819\n",
      "Epoch 2: saving model to model_init_2022-05-16--17_28_33.805466\\model-00002-6.99736-0.98190-7.61347-0.78000.h5\n",
      "34/34 [==============================] - 225s 7s/step - loss: 6.9974 - categorical_accuracy: 0.9819 - val_loss: 7.6135 - val_categorical_accuracy: 0.7800 - lr: 0.0100\n",
      "Epoch 3/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 6.8795 - categorical_accuracy: 0.9940\n",
      "Epoch 3: saving model to model_init_2022-05-16--17_28_33.805466\\model-00003-6.87951-0.99397-7.66171-0.64000.h5\n",
      "34/34 [==============================] - 234s 7s/step - loss: 6.8795 - categorical_accuracy: 0.9940 - val_loss: 7.6617 - val_categorical_accuracy: 0.6400 - lr: 0.0100\n",
      "Epoch 4/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 6.8332 - categorical_accuracy: 0.9894\n",
      "Epoch 4: saving model to model_init_2022-05-16--17_28_33.805466\\model-00004-6.83324-0.98944-7.41076-0.79000.h5\n",
      "34/34 [==============================] - 238s 7s/step - loss: 6.8332 - categorical_accuracy: 0.9894 - val_loss: 7.4108 - val_categorical_accuracy: 0.7900 - lr: 0.0020\n",
      "Epoch 5/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 6.8033 - categorical_accuracy: 0.9940\n",
      "Epoch 5: saving model to model_init_2022-05-16--17_28_33.805466\\model-00005-6.80329-0.99397-7.39985-0.76000.h5\n",
      "34/34 [==============================] - 241s 7s/step - loss: 6.8033 - categorical_accuracy: 0.9940 - val_loss: 7.3998 - val_categorical_accuracy: 0.7600 - lr: 0.0020\n",
      "Epoch 6/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 6.7752 - categorical_accuracy: 0.9985\n",
      "Epoch 6: saving model to model_init_2022-05-16--17_28_33.805466\\model-00006-6.77517-0.99849-7.28786-0.80000.h5\n",
      "34/34 [==============================] - 246s 7s/step - loss: 6.7752 - categorical_accuracy: 0.9985 - val_loss: 7.2879 - val_categorical_accuracy: 0.8000 - lr: 0.0020\n",
      "Epoch 7/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 6.7661 - categorical_accuracy: 0.9970\n",
      "Epoch 7: saving model to model_init_2022-05-16--17_28_33.805466\\model-00007-6.76609-0.99698-7.18996-0.83000.h5\n",
      "34/34 [==============================] - 241s 7s/step - loss: 6.7661 - categorical_accuracy: 0.9970 - val_loss: 7.1900 - val_categorical_accuracy: 0.8300 - lr: 0.0020\n",
      "Epoch 8/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 6.7366 - categorical_accuracy: 0.9985\n",
      "Epoch 8: saving model to model_init_2022-05-16--17_28_33.805466\\model-00008-6.73655-0.99849-7.33282-0.79000.h5\n",
      "34/34 [==============================] - 245s 7s/step - loss: 6.7366 - categorical_accuracy: 0.9985 - val_loss: 7.3328 - val_categorical_accuracy: 0.7900 - lr: 0.0020\n",
      "Epoch 9/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 6.7243 - categorical_accuracy: 0.9985\n",
      "Epoch 9: saving model to model_init_2022-05-16--17_28_33.805466\\model-00009-6.72428-0.99849-7.19685-0.80000.h5\n",
      "34/34 [==============================] - 245s 7s/step - loss: 6.7243 - categorical_accuracy: 0.9985 - val_loss: 7.1969 - val_categorical_accuracy: 0.8000 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 6.7103 - categorical_accuracy: 0.9985\n",
      "Epoch 10: saving model to model_init_2022-05-16--17_28_33.805466\\model-00010-6.71031-0.99849-7.10908-0.82000.h5\n",
      "34/34 [==============================] - 244s 7s/step - loss: 6.7103 - categorical_accuracy: 0.9985 - val_loss: 7.1091 - val_categorical_accuracy: 0.8200 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 6.7015 - categorical_accuracy: 1.0000\n",
      "Epoch 11: saving model to model_init_2022-05-16--17_28_33.805466\\model-00011-6.70151-1.00000-7.12994-0.83000.h5\n",
      "34/34 [==============================] - 243s 7s/step - loss: 6.7015 - categorical_accuracy: 1.0000 - val_loss: 7.1299 - val_categorical_accuracy: 0.8300 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 6.6915 - categorical_accuracy: 0.9970\n",
      "Epoch 12: saving model to model_init_2022-05-16--17_28_33.805466\\model-00012-6.69147-0.99698-7.15914-0.81000.h5\n",
      "34/34 [==============================] - 248s 7s/step - loss: 6.6915 - categorical_accuracy: 0.9970 - val_loss: 7.1591 - val_categorical_accuracy: 0.8100 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 6.6797 - categorical_accuracy: 1.0000\n",
      "Epoch 13: saving model to model_init_2022-05-16--17_28_33.805466\\model-00013-6.67974-1.00000-7.17150-0.82000.h5\n",
      "34/34 [==============================] - 244s 7s/step - loss: 6.6797 - categorical_accuracy: 1.0000 - val_loss: 7.1715 - val_categorical_accuracy: 0.8200 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 6.6740 - categorical_accuracy: 0.9985\n",
      "Epoch 14: saving model to model_init_2022-05-16--17_28_33.805466\\model-00014-6.67403-0.99849-7.02668-0.85000.h5\n",
      "34/34 [==============================] - 242s 7s/step - loss: 6.6740 - categorical_accuracy: 0.9985 - val_loss: 7.0267 - val_categorical_accuracy: 0.8500 - lr: 0.0010\n",
      "Epoch 15/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 6.6626 - categorical_accuracy: 1.0000\n",
      "Epoch 15: saving model to model_init_2022-05-16--17_28_33.805466\\model-00015-6.66255-1.00000-7.06810-0.84000.h5\n",
      "34/34 [==============================] - 243s 7s/step - loss: 6.6626 - categorical_accuracy: 1.0000 - val_loss: 7.0681 - val_categorical_accuracy: 0.8400 - lr: 0.0010\n",
      "Epoch 16/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 6.6532 - categorical_accuracy: 0.9985\n",
      "Epoch 16: saving model to model_init_2022-05-16--17_28_33.805466\\model-00016-6.65321-0.99849-7.06796-0.84000.h5\n",
      "34/34 [==============================] - 244s 7s/step - loss: 6.6532 - categorical_accuracy: 0.9985 - val_loss: 7.0680 - val_categorical_accuracy: 0.8400 - lr: 0.0010\n",
      "Epoch 17/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 6.6430 - categorical_accuracy: 1.0000\n",
      "Epoch 17: saving model to model_init_2022-05-16--17_28_33.805466\\model-00017-6.64303-1.00000-7.16477-0.82000.h5\n",
      "34/34 [==============================] - 244s 7s/step - loss: 6.6430 - categorical_accuracy: 1.0000 - val_loss: 7.1648 - val_categorical_accuracy: 0.8200 - lr: 0.0010\n",
      "Epoch 18/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 6.6344 - categorical_accuracy: 1.0000\n",
      "Epoch 18: saving model to model_init_2022-05-16--17_28_33.805466\\model-00018-6.63439-1.00000-7.05284-0.82000.h5\n",
      "34/34 [==============================] - 244s 7s/step - loss: 6.6344 - categorical_accuracy: 1.0000 - val_loss: 7.0528 - val_categorical_accuracy: 0.8200 - lr: 0.0010\n",
      "Epoch 19/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 6.6311 - categorical_accuracy: 0.9985\n",
      "Epoch 19: saving model to model_init_2022-05-16--17_28_33.805466\\model-00019-6.63112-0.99849-7.07086-0.83000.h5\n",
      "34/34 [==============================] - 248s 7s/step - loss: 6.6311 - categorical_accuracy: 0.9985 - val_loss: 7.0709 - val_categorical_accuracy: 0.8300 - lr: 0.0010\n",
      "Epoch 20/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 6.6168 - categorical_accuracy: 0.9985\n",
      "Epoch 20: saving model to model_init_2022-05-16--17_28_33.805466\\model-00020-6.61677-0.99849-7.05304-0.83000.h5\n",
      "34/34 [==============================] - 244s 7s/step - loss: 6.6168 - categorical_accuracy: 0.9985 - val_loss: 7.0530 - val_categorical_accuracy: 0.8300 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bf159d9690>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now let's again check with decreasing learning rate at each epoch (patience = 1)\n",
    "batch_size = 20\n",
    "num_epochs = 20\n",
    "train_generator = generator(train_path, train_doc, batch_size,scale_f, n_i)\n",
    "val_generator = generator(val_path, val_doc, batch_size,scale_f, n_i)\n",
    "\n",
    "#save model weights\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','--').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=1, min_lr=0.001) #Reducelronplateau code\n",
    "callbacks_list = [checkpoint, LR]\n",
    "\n",
    "\n",
    "#steps per epoch\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1\n",
    "    \n",
    "#fit model\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings\n",
    "* Model is still overfitting.So we will use transfer learning in the next model\n",
    "- Train loss: 6.6168 \n",
    "- Train categorical_accuracy: 0.9985 \n",
    "- val_loss: 7.0530\n",
    "- val_categorical_accuracy: 0.8300 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 5\n",
    " * We should use Resnet50 for transfer learning\n",
    " * We kept the learning rate at 0.0004"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed_50 (TimeDi  (None, 20, 7, 7, 2048)   23587712  \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_51 (TimeDi  (None, 20, 2048)         0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_52 (TimeDi  (None, 20, 2048)         0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " gru_4 (GRU)                 (None, 20, 1000)          9150000   \n",
      "                                                                 \n",
      " time_distributed_53 (TimeDi  (None, 20, 100)          100100    \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_54 (TimeDi  (None, 20, 100)          0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " flatten_8 (Flatten)         (None, 2000)              0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 64)                128064    \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 32,966,201\n",
      "Trainable params: 19,368,633\n",
      "Non-trainable params: 13,597,568\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#let's try transfer learning with Resnet\n",
    "scale_f = 197  #size of image limitation with Resnet is 197\n",
    "n_i = 20\n",
    "\n",
    "# create a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(scale_f, scale_f, 3))\n",
    "\n",
    "#making some layers trainable\n",
    "split_at = 150\n",
    "for layer in base_model.layers[:split_at]: layer.trainable = False\n",
    "for layer in base_model.layers[split_at:]: layer.trainable = True\n",
    "\n",
    "model.add(TimeDistributed(base_model, input_shape=(n_i, scale_f, scale_f, 3)))\n",
    "model.add(TimeDistributed(GlobalAveragePooling2D()))\n",
    "# flatten\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "#adding GRU/LSTM layers\n",
    "model.add(GRU(1000,return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(100, activation='relu')))\n",
    "model.add(TimeDistributed(Dropout(0.30)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "optimiser = 'sgd' #optimizer\n",
    "\n",
    "input_param = (None, n_i, scale_f,scale_f,3)\n",
    "model.build(input_param)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and Fit the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Source path =  train ; batch size = 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\44775\\AppData\\Local\\Temp\\ipykernel_13840\\1976619657.py:40: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.6212 - categorical_accuracy: 0.2293 Source path =  val ; batch size = 20\n",
      "\n",
      "Epoch 1: saving model to model_init_2022-05-16--17_28_33.805466\\model-00001-1.62121-0.22926-1.51477-0.32000.h5\n",
      "34/34 [==============================] - 1043s 31s/step - loss: 1.6212 - categorical_accuracy: 0.2293 - val_loss: 1.5148 - val_categorical_accuracy: 0.3200 - lr: 4.0000e-04\n",
      "Epoch 2/15\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.4237 - categorical_accuracy: 0.4072 \n",
      "Epoch 2: saving model to model_init_2022-05-16--17_28_33.805466\\model-00002-1.42372-0.40724-1.19059-0.65000.h5\n",
      "34/34 [==============================] - 1032s 30s/step - loss: 1.4237 - categorical_accuracy: 0.4072 - val_loss: 1.1906 - val_categorical_accuracy: 0.6500 - lr: 4.0000e-04\n",
      "Epoch 3/15\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.0678 - categorical_accuracy: 0.6139 \n",
      "Epoch 3: saving model to model_init_2022-05-16--17_28_33.805466\\model-00003-1.06775-0.61388-0.78545-0.71000.h5\n",
      "34/34 [==============================] - 1032s 30s/step - loss: 1.0678 - categorical_accuracy: 0.6139 - val_loss: 0.7854 - val_categorical_accuracy: 0.7100 - lr: 4.0000e-04\n",
      "Epoch 4/15\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.7410 - categorical_accuracy: 0.7315 \n",
      "Epoch 4: saving model to model_init_2022-05-16--17_28_33.805466\\model-00004-0.74096-0.73152-0.54289-0.85000.h5\n",
      "34/34 [==============================] - 1033s 30s/step - loss: 0.7410 - categorical_accuracy: 0.7315 - val_loss: 0.5429 - val_categorical_accuracy: 0.8500 - lr: 4.0000e-04\n",
      "Epoch 5/15\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.4502 - categorical_accuracy: 0.8597 \n",
      "Epoch 5: saving model to model_init_2022-05-16--17_28_33.805466\\model-00005-0.45020-0.85973-0.32964-0.95000.h5\n",
      "34/34 [==============================] - 1011s 30s/step - loss: 0.4502 - categorical_accuracy: 0.8597 - val_loss: 0.3296 - val_categorical_accuracy: 0.9500 - lr: 4.0000e-04\n",
      "Epoch 6/15\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.3198 - categorical_accuracy: 0.9125 \n",
      "Epoch 6: saving model to model_init_2022-05-16--17_28_33.805466\\model-00006-0.31983-0.91252-0.23224-0.95000.h5\n",
      "34/34 [==============================] - 975s 29s/step - loss: 0.3198 - categorical_accuracy: 0.9125 - val_loss: 0.2322 - val_categorical_accuracy: 0.9500 - lr: 4.0000e-04\n",
      "Epoch 7/15\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2477 - categorical_accuracy: 0.9261 \n",
      "Epoch 7: saving model to model_init_2022-05-16--17_28_33.805466\\model-00007-0.24765-0.92609-0.18201-0.96000.h5\n",
      "34/34 [==============================] - 1033s 30s/step - loss: 0.2477 - categorical_accuracy: 0.9261 - val_loss: 0.1820 - val_categorical_accuracy: 0.9600 - lr: 4.0000e-04\n",
      "Epoch 8/15\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.1774 - categorical_accuracy: 0.9623 \n",
      "Epoch 8: saving model to model_init_2022-05-16--17_28_33.805466\\model-00008-0.17742-0.96229-0.15479-0.97000.h5\n",
      "34/34 [==============================] - 1032s 30s/step - loss: 0.1774 - categorical_accuracy: 0.9623 - val_loss: 0.1548 - val_categorical_accuracy: 0.9700 - lr: 4.0000e-04\n",
      "Epoch 9/15\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.1201 - categorical_accuracy: 0.9713 \n",
      "Epoch 9: saving model to model_init_2022-05-16--17_28_33.805466\\model-00009-0.12014-0.97134-0.10599-0.98000.h5\n",
      "34/34 [==============================] - 1027s 30s/step - loss: 0.1201 - categorical_accuracy: 0.9713 - val_loss: 0.1060 - val_categorical_accuracy: 0.9800 - lr: 4.0000e-04\n",
      "Epoch 10/15\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.1049 - categorical_accuracy: 0.9774 \n",
      "Epoch 10: saving model to model_init_2022-05-16--17_28_33.805466\\model-00010-0.10486-0.97738-0.09248-0.97000.h5\n",
      "34/34 [==============================] - 1021s 30s/step - loss: 0.1049 - categorical_accuracy: 0.9774 - val_loss: 0.0925 - val_categorical_accuracy: 0.9700 - lr: 4.0000e-04\n",
      "Epoch 11/15\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0743 - categorical_accuracy: 0.9834 \n",
      "Epoch 11: saving model to model_init_2022-05-16--17_28_33.805466\\model-00011-0.07430-0.98341-0.09377-0.98000.h5\n",
      "34/34 [==============================] - 1031s 30s/step - loss: 0.0743 - categorical_accuracy: 0.9834 - val_loss: 0.0938 - val_categorical_accuracy: 0.9800 - lr: 4.0000e-04\n",
      "Epoch 12/15\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0533 - categorical_accuracy: 0.9940 \n",
      "Epoch 12: saving model to model_init_2022-05-16--17_28_33.805466\\model-00012-0.05329-0.99397-0.07115-0.99000.h5\n",
      "34/34 [==============================] - 1022s 30s/step - loss: 0.0533 - categorical_accuracy: 0.9940 - val_loss: 0.0711 - val_categorical_accuracy: 0.9900 - lr: 4.0000e-04\n",
      "Epoch 13/15\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0505 - categorical_accuracy: 0.9925 \n",
      "Epoch 13: saving model to model_init_2022-05-16--17_28_33.805466\\model-00013-0.05045-0.99246-0.08764-0.98000.h5\n",
      "34/34 [==============================] - 1034s 30s/step - loss: 0.0505 - categorical_accuracy: 0.9925 - val_loss: 0.0876 - val_categorical_accuracy: 0.9800 - lr: 4.0000e-04\n",
      "Epoch 14/15\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0387 - categorical_accuracy: 0.9925 \n",
      "Epoch 14: saving model to model_init_2022-05-16--17_28_33.805466\\model-00014-0.03871-0.99246-0.07234-0.99000.h5\n",
      "34/34 [==============================] - 1023s 30s/step - loss: 0.0387 - categorical_accuracy: 0.9925 - val_loss: 0.0723 - val_categorical_accuracy: 0.9900 - lr: 4.0000e-04\n",
      "Epoch 15/15\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0312 - categorical_accuracy: 0.9970 \n",
      "Epoch 15: saving model to model_init_2022-05-16--17_28_33.805466\\model-00015-0.03121-0.99698-0.05134-0.99000.h5\n",
      "34/34 [==============================] - 1030s 30s/step - loss: 0.0312 - categorical_accuracy: 0.9970 - val_loss: 0.0513 - val_categorical_accuracy: 0.9900 - lr: 4.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bf48808eb0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#increasing epochs and decreasing learning rate to 0.0004\n",
    "\n",
    "batch_size = 20\n",
    "num_epochs = 15\n",
    "train_generator = generator(train_path, train_doc, batch_size,scale_f, n_i)\n",
    "val_generator = generator(val_path, val_doc, batch_size,scale_f, n_i)\n",
    "\n",
    "#save model weights\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','--').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=1, min_lr=0.0004) #Reducelronplateau code\n",
    "callbacks_list = [checkpoint, LR]\n",
    "\n",
    "\n",
    "#steps per epoch\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1\n",
    "    \n",
    "#fit model\n",
    "# Choosing lower learning rate for fine-tuning\n",
    "# learning rate is generally 10-1000 times lower than normal learning rate when we are fine tuning the initial layers\n",
    "sgd = tf.keras.optimizers.SGD(lr=0.0004, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['categorical_accuracy'])\n",
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings\n",
    "* Resnet50 model has managed the overfitting well. Transfer learning has been successful.\n",
    "- Train loss: 0.0312 \n",
    "- Train categorical_accuracy: 0.9970 \n",
    "- val_loss: 0.0513 \n",
    "- val_categorical_accuracy: 0.9900 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 6\n",
    "\n",
    " * now we should fine tune this model more,  train over 140th layer\n",
    " * we also changing GRU layer to LSTM\n",
    " * We kept the learning rate at 0.0005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed_55 (TimeDi  (None, 20, 7, 7, 2048)   23587712  \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_56 (TimeDi  (None, 20, 2048)         0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_57 (TimeDi  (None, 20, 2048)         0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 20, 1024)          12587008  \n",
      "                                                                 \n",
      " time_distributed_58 (TimeDi  (None, 20, 128)          131200    \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_59 (TimeDi  (None, 20, 128)          0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " flatten_10 (Flatten)        (None, 2560)              0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 64)                163904    \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 36,470,149\n",
      "Trainable params: 27,860,485\n",
      "Non-trainable params: 8,609,664\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "scale_f = 197 #size of image limitation with Resnet\n",
    "n_i = 20\n",
    "\n",
    "# create a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(scale_f, scale_f, 3))\n",
    "\n",
    "#making some layers trainable\n",
    "split_at = 140\n",
    "for layer in base_model.layers[:split_at]: layer.trainable = False\n",
    "for layer in base_model.layers[split_at:]: layer.trainable = True\n",
    "\n",
    "model.add(TimeDistributed(base_model, input_shape=(n_i, scale_f, scale_f, 3)))\n",
    "model.add(TimeDistributed(GlobalAveragePooling2D()))\n",
    "# flatten\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "#adding GRU/LSTM layers of more units and adding timedistributed dense layer of 128 neurons each\n",
    "model.add(LSTM(1024,return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(128, activation='relu')))\n",
    "model.add(TimeDistributed(Dropout(0.30)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "optimiser = 'sgd' #optimizer\n",
    "\n",
    "input_param = (None, n_i, scale_f,scale_f,3)\n",
    "model.build(input_param)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and Fit the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Source path =  train ; batch size = 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\44775\\AppData\\Local\\Temp\\ipykernel_13840\\1777622456.py:40: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.5662 - categorical_accuracy: 0.2821 Source path =  val ; batch size = 20\n",
      "\n",
      "Epoch 1: saving model to model_init_2022-05-16--17_28_33.805466\\model-00001-1.56621-0.28205-1.42013-0.48000.h5\n",
      "34/34 [==============================] - 1145s 34s/step - loss: 1.5662 - categorical_accuracy: 0.2821 - val_loss: 1.4201 - val_categorical_accuracy: 0.4800 - lr: 5.0000e-04\n",
      "Epoch 2/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.1777 - categorical_accuracy: 0.6018 \n",
      "Epoch 2: saving model to model_init_2022-05-16--17_28_33.805466\\model-00002-1.17772-0.60181-0.94130-0.73000.h5\n",
      "34/34 [==============================] - 1148s 34s/step - loss: 1.1777 - categorical_accuracy: 0.6018 - val_loss: 0.9413 - val_categorical_accuracy: 0.7300 - lr: 5.0000e-04\n",
      "Epoch 3/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.7295 - categorical_accuracy: 0.7526 \n",
      "Epoch 3: saving model to model_init_2022-05-16--17_28_33.805466\\model-00003-0.72950-0.75264-0.58495-0.77000.h5\n",
      "34/34 [==============================] - 1135s 33s/step - loss: 0.7295 - categorical_accuracy: 0.7526 - val_loss: 0.5850 - val_categorical_accuracy: 0.7700 - lr: 5.0000e-04\n",
      "Epoch 4/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.4216 - categorical_accuracy: 0.8673 \n",
      "Epoch 4: saving model to model_init_2022-05-16--17_28_33.805466\\model-00004-0.42156-0.86727-0.37696-0.86000.h5\n",
      "34/34 [==============================] - 1133s 33s/step - loss: 0.4216 - categorical_accuracy: 0.8673 - val_loss: 0.3770 - val_categorical_accuracy: 0.8600 - lr: 5.0000e-04\n",
      "Epoch 5/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2697 - categorical_accuracy: 0.9246 \n",
      "Epoch 5: saving model to model_init_2022-05-16--17_28_33.805466\\model-00005-0.26966-0.92459-0.26579-0.93000.h5\n",
      "34/34 [==============================] - 1141s 34s/step - loss: 0.2697 - categorical_accuracy: 0.9246 - val_loss: 0.2658 - val_categorical_accuracy: 0.9300 - lr: 5.0000e-04\n",
      "Epoch 6/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.1633 - categorical_accuracy: 0.9653 \n",
      "Epoch 6: saving model to model_init_2022-05-16--17_28_33.805466\\model-00006-0.16328-0.96531-0.17579-0.96000.h5\n",
      "34/34 [==============================] - 1132s 33s/step - loss: 0.1633 - categorical_accuracy: 0.9653 - val_loss: 0.1758 - val_categorical_accuracy: 0.9600 - lr: 5.0000e-04\n",
      "Epoch 7/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.1137 - categorical_accuracy: 0.9744 \n",
      "Epoch 7: saving model to model_init_2022-05-16--17_28_33.805466\\model-00007-0.11367-0.97436-0.14092-0.94000.h5\n",
      "34/34 [==============================] - 1140s 34s/step - loss: 0.1137 - categorical_accuracy: 0.9744 - val_loss: 0.1409 - val_categorical_accuracy: 0.9400 - lr: 5.0000e-04\n",
      "Epoch 8/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0780 - categorical_accuracy: 0.9849 \n",
      "Epoch 8: saving model to model_init_2022-05-16--17_28_33.805466\\model-00008-0.07802-0.98492-0.10795-0.96000.h5\n",
      "34/34 [==============================] - 1134s 33s/step - loss: 0.0780 - categorical_accuracy: 0.9849 - val_loss: 0.1080 - val_categorical_accuracy: 0.9600 - lr: 5.0000e-04\n",
      "Epoch 9/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0559 - categorical_accuracy: 0.9894 \n",
      "Epoch 9: saving model to model_init_2022-05-16--17_28_33.805466\\model-00009-0.05595-0.98944-0.20785-0.91000.h5\n",
      "34/34 [==============================] - 1137s 34s/step - loss: 0.0559 - categorical_accuracy: 0.9894 - val_loss: 0.2078 - val_categorical_accuracy: 0.9100 - lr: 5.0000e-04\n",
      "Epoch 10/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0826 - categorical_accuracy: 0.9819 \n",
      "Epoch 10: saving model to model_init_2022-05-16--17_28_33.805466\\model-00010-0.08261-0.98190-0.08627-0.98000.h5\n",
      "34/34 [==============================] - 1133s 33s/step - loss: 0.0826 - categorical_accuracy: 0.9819 - val_loss: 0.0863 - val_categorical_accuracy: 0.9800 - lr: 5.0000e-04\n",
      "Epoch 11/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0403 - categorical_accuracy: 0.9970 \n",
      "Epoch 11: saving model to model_init_2022-05-16--17_28_33.805466\\model-00011-0.04030-0.99698-0.08515-0.99000.h5\n",
      "34/34 [==============================] - 1145s 34s/step - loss: 0.0403 - categorical_accuracy: 0.9970 - val_loss: 0.0852 - val_categorical_accuracy: 0.9900 - lr: 5.0000e-04\n",
      "Epoch 12/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0378 - categorical_accuracy: 0.9925 \n",
      "Epoch 12: saving model to model_init_2022-05-16--17_28_33.805466\\model-00012-0.03781-0.99246-0.07874-0.97000.h5\n",
      "34/34 [==============================] - 1143s 34s/step - loss: 0.0378 - categorical_accuracy: 0.9925 - val_loss: 0.0787 - val_categorical_accuracy: 0.9700 - lr: 5.0000e-04\n",
      "Epoch 13/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0262 - categorical_accuracy: 0.9955 \n",
      "Epoch 13: saving model to model_init_2022-05-16--17_28_33.805466\\model-00013-0.02621-0.99548-0.04377-1.00000.h5\n",
      "34/34 [==============================] - 1153s 34s/step - loss: 0.0262 - categorical_accuracy: 0.9955 - val_loss: 0.0438 - val_categorical_accuracy: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 14/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0176 - categorical_accuracy: 0.9985 \n",
      "Epoch 14: saving model to model_init_2022-05-16--17_28_33.805466\\model-00014-0.01765-0.99849-0.06719-0.98000.h5\n",
      "34/34 [==============================] - 1144s 34s/step - loss: 0.0176 - categorical_accuracy: 0.9985 - val_loss: 0.0672 - val_categorical_accuracy: 0.9800 - lr: 5.0000e-04\n",
      "Epoch 15/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0158 - categorical_accuracy: 0.9985 \n",
      "Epoch 15: saving model to model_init_2022-05-16--17_28_33.805466\\model-00015-0.01583-0.99849-0.04919-0.99000.h5\n",
      "34/34 [==============================] - 1157s 34s/step - loss: 0.0158 - categorical_accuracy: 0.9985 - val_loss: 0.0492 - val_categorical_accuracy: 0.9900 - lr: 5.0000e-04\n",
      "Epoch 16/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0125 - categorical_accuracy: 1.0000 \n",
      "Epoch 16: saving model to model_init_2022-05-16--17_28_33.805466\\model-00016-0.01251-1.00000-0.03923-0.99000.h5\n",
      "34/34 [==============================] - 1154s 34s/step - loss: 0.0125 - categorical_accuracy: 1.0000 - val_loss: 0.0392 - val_categorical_accuracy: 0.9900 - lr: 5.0000e-04\n",
      "Epoch 17/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0144 - categorical_accuracy: 0.9985 \n",
      "Epoch 17: saving model to model_init_2022-05-16--17_28_33.805466\\model-00017-0.01444-0.99849-0.06332-0.98000.h5\n",
      "34/34 [==============================] - 1175s 35s/step - loss: 0.0144 - categorical_accuracy: 0.9985 - val_loss: 0.0633 - val_categorical_accuracy: 0.9800 - lr: 5.0000e-04\n",
      "Epoch 18/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0092 - categorical_accuracy: 1.0000 \n",
      "Epoch 18: saving model to model_init_2022-05-16--17_28_33.805466\\model-00018-0.00917-1.00000-0.02913-1.00000.h5\n",
      "34/34 [==============================] - 1167s 34s/step - loss: 0.0092 - categorical_accuracy: 1.0000 - val_loss: 0.0291 - val_categorical_accuracy: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 19/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0103 - categorical_accuracy: 0.9985 \n",
      "Epoch 19: saving model to model_init_2022-05-16--17_28_33.805466\\model-00019-0.01033-0.99849-0.05949-0.99000.h5\n",
      "34/34 [==============================] - 1087s 32s/step - loss: 0.0103 - categorical_accuracy: 0.9985 - val_loss: 0.0595 - val_categorical_accuracy: 0.9900 - lr: 5.0000e-04\n",
      "Epoch 20/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0071 - categorical_accuracy: 1.0000 \n",
      "Epoch 20: saving model to model_init_2022-05-16--17_28_33.805466\\model-00020-0.00713-1.00000-0.04419-0.98000.h5\n",
      "34/34 [==============================] - 1050s 31s/step - loss: 0.0071 - categorical_accuracy: 1.0000 - val_loss: 0.0442 - val_categorical_accuracy: 0.9800 - lr: 5.0000e-04\n",
      "Epoch 21/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0073 - categorical_accuracy: 1.0000 \n",
      "Epoch 21: saving model to model_init_2022-05-16--17_28_33.805466\\model-00021-0.00731-1.00000-0.03898-0.99000.h5\n",
      "34/34 [==============================] - 1054s 31s/step - loss: 0.0073 - categorical_accuracy: 1.0000 - val_loss: 0.0390 - val_categorical_accuracy: 0.9900 - lr: 5.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0080 - categorical_accuracy: 1.0000 \n",
      "Epoch 22: saving model to model_init_2022-05-16--17_28_33.805466\\model-00022-0.00799-1.00000-0.04621-0.99000.h5\n",
      "34/34 [==============================] - 1059s 31s/step - loss: 0.0080 - categorical_accuracy: 1.0000 - val_loss: 0.0462 - val_categorical_accuracy: 0.9900 - lr: 5.0000e-04\n",
      "Epoch 23/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0104 - categorical_accuracy: 0.9970 \n",
      "Epoch 23: saving model to model_init_2022-05-16--17_28_33.805466\\model-00023-0.01037-0.99698-0.04019-0.99000.h5\n",
      "34/34 [==============================] - 1025s 30s/step - loss: 0.0104 - categorical_accuracy: 0.9970 - val_loss: 0.0402 - val_categorical_accuracy: 0.9900 - lr: 5.0000e-04\n",
      "Epoch 24/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0061 - categorical_accuracy: 1.0000 \n",
      "Epoch 24: saving model to model_init_2022-05-16--17_28_33.805466\\model-00024-0.00609-1.00000-0.05215-0.98000.h5\n",
      "34/34 [==============================] - 1024s 30s/step - loss: 0.0061 - categorical_accuracy: 1.0000 - val_loss: 0.0522 - val_categorical_accuracy: 0.9800 - lr: 5.0000e-04\n",
      "Epoch 25/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0050 - categorical_accuracy: 1.0000 \n",
      "Epoch 25: saving model to model_init_2022-05-16--17_28_33.805466\\model-00025-0.00496-1.00000-0.00877-1.00000.h5\n",
      "34/34 [==============================] - 1022s 30s/step - loss: 0.0050 - categorical_accuracy: 1.0000 - val_loss: 0.0088 - val_categorical_accuracy: 1.0000 - lr: 5.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bf71e301f0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#increasing epochs to 25 and making learning rate to 0.0005\n",
    "\n",
    "batch_size = 20\n",
    "num_epochs = 25\n",
    "train_generator = generator(train_path, train_doc, batch_size,scale_f, n_i)\n",
    "val_generator = generator(val_path, val_doc, batch_size,scale_f, n_i)\n",
    "\n",
    "#save model weights\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','--').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=1, min_lr=0.0005) #Reducelronplateau code\n",
    "callbacks_list = [checkpoint, LR]\n",
    "\n",
    "\n",
    "#steps per epoch\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1\n",
    "    \n",
    "#fit model\n",
    "# Choosing learning rate of 0.0005\n",
    "\n",
    "sgd = tf.keras.optimizers.SGD(lr=0.0005, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['categorical_accuracy'])\n",
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings:\n",
    " * Perfect accuracy on train and valid data. Even Loss has reduced.\n",
    "- Train loss: 0.0050 \n",
    "- Train categorical_accuracy: 1.0000 \n",
    "- val_loss: 0.0088 \n",
    "- val_categorical_accuracy: 1.0000 \n",
    " *        \n",
    "    Total params: 36,470,149\n",
    "    Trainable params: 27,860,485\n",
    "    Non-trainable params: 8,609,664"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
