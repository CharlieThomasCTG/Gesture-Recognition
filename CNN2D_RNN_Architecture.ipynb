{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gesture Recognition\n",
    "In this group project, you are going to build a 2D CNN + RNN model that will be able to predict the 5 gestures correctly. Please import the following libraries to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "#we will use cv2 here\n",
    "import cv2\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.applications.resnet import ResNet50\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, LSTM, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation, Dropout, GlobalAveragePooling2D\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D, Conv2D, MaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the random seed so that the results don't vary drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "663 100\n"
     ]
    }
   ],
   "source": [
    "train_doc = np.random.permutation(open('train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('val.csv').readlines())\n",
    "print(len(train_doc), len(val_doc))\n",
    "batch_size = 50 #experiment with the batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "This is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with `img_idx`, `y`,`z` and normalization such that you get high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_f = 70"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have modified the generator function by adding 2 more parameters scale_f -> size of image to be rescaled and n_i -> number of images to train on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(source_path, folder_list, batch_size, scale_f, n_i):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    #create a list of image numbers you want to use for a particular video\n",
    "    img_idx = [i for i in range(30)]\n",
    "    if(n_i == 20):\n",
    "        img_idx = [0,1,4,5,7,8,10,11,13,14,16,17,19,20,22,23,25,26,28,29]\n",
    "    elif(n_i == 15):\n",
    "        img_idx = [i for i in range(0,30,2)]\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        # calculate the number of batches\n",
    "        num_batches = int(len(folder_list)/batch_size)\n",
    "    \n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,len(img_idx),scale_f,scale_f,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = cv2.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    \n",
    "                    #We have 160*120 and 360*360 images, we will resize every image to scale_f*scale_f\n",
    "                    image = cv2.resize(image, (scale_f, scale_f))\n",
    "                    \n",
    "                    \n",
    "                    #normalising using min-max method\n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0] - np.min(image[:,:,0])/np.max(image[:,:,0]) - np.min(image[:,:,0])#normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1] - np.min(image[:,:,1])/np.max(image[:,:,1]) - np.min(image[:,:,1])#normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2] - np.min(image[:,:,2])/np.max(image[:,:,2]) - np.min(image[:,:,2])#normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        \n",
    "        left_samples = len(folder_list)%batch_size\n",
    "        offset = len(folder_list) - left_samples\n",
    "        if (left_samples > 0):\n",
    "            batch_data = np.zeros((left_samples,len(img_idx),scale_f,scale_f,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((left_samples,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(left_samples): # iterate over the left_samples\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + offset].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = cv2.imread(source_path+'/'+ t[folder + offset].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "\n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "\n",
    "                    #We have 160*120 and 360*360 images, we will resize every image to scale_f*scale_f\n",
    "                    image = cv2.resize(image, (scale_f, scale_f))\n",
    "\n",
    "\n",
    "                    #normalising using min-max method\n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0] - np.min(image[:,:,0])/np.max(image[:,:,0]) - np.min(image[:,:,0])#normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1] - np.min(image[:,:,1])/np.max(image[:,:,1]) - np.min(image[:,:,1])#normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2] - np.min(image[:,:,2])/np.max(image[:,:,2]) - np.min(image[:,:,2])#normalise and feed in the image\n",
    "\n",
    "                batch_labels[folder, int(t[folder + offset].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here that a video is represented above in the generator as (number of images, height, width, number of channels). Take this into consideration while creating the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 1\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = 'train'\n",
    "val_path = 'val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 1# choose the number of epochs\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1\n",
    "We will make the model using different functionalities that Keras provides. We will use `TimeDistributed` while building a Conv2D + RNN model.The last layer is the softmax. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(30)\n",
    "#initially building a simple model to test whether model is fitting and generator is running or not\n",
    "model = Sequential()\n",
    "n_i = 30\n",
    "model.add(TimeDistributed(Conv2D(64,(3,3),activation='relu',input_shape=(n_i, scale_f, scale_f, 3))))#(inputs))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "model.add(TimeDistributed(Dropout(0.25)))\n",
    "\n",
    "# flatten\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "#addig GRU layers\n",
    "model.add(GRU(64))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed (TimeDistr  (None, 30, 68, 68, 64)   1792      \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDis  (None, 30, 34, 34, 64)   0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_2 (TimeDis  (None, 30, 34, 34, 64)   0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_3 (TimeDis  (None, 30, 73984)        0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 64)                14217600  \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               6500      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 505       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,226,397\n",
      "Trainable params: 14,226,397\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser = 'sgd'#optimizer\n",
    "\n",
    "#building model with input\n",
    "input_param = (None, n_i, scale_f,scale_f,3)\n",
    "model.build(input_param)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size, scale_f, n_i)\n",
    "val_generator = generator(val_path, val_doc, batch_size, scale_f, n_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','--').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=1, min_lr=0.001) # Reducelronplateau code\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and Fit the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  train ; batch size = 50\n",
      "Epoch 1/10\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.7681 - categorical_accuracy: 0.2112Source path =  val ; batch size = 50\n",
      "\n",
      "Epoch 1: saving model to model_init_2022-05-17--18_12_33.848004\\model-00001-1.76814-0.21116-1.65975-0.16000.h5\n",
      "14/14 [==============================] - 66s 5s/step - loss: 1.7681 - categorical_accuracy: 0.2112 - val_loss: 1.6597 - val_categorical_accuracy: 0.1600 - lr: 0.0100\n",
      "Epoch 2/10\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.6820 - categorical_accuracy: 0.1885\n",
      "Epoch 2: saving model to model_init_2022-05-17--18_12_33.848004\\model-00002-1.68203-0.18854-1.65610-0.14000.h5\n",
      "14/14 [==============================] - 62s 4s/step - loss: 1.6820 - categorical_accuracy: 0.1885 - val_loss: 1.6561 - val_categorical_accuracy: 0.1400 - lr: 0.0100\n",
      "Epoch 3/10\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.6529 - categorical_accuracy: 0.1900\n",
      "Epoch 3: saving model to model_init_2022-05-17--18_12_33.848004\\model-00003-1.65289-0.19005-1.61355-0.16000.h5\n",
      "14/14 [==============================] - 62s 5s/step - loss: 1.6529 - categorical_accuracy: 0.1900 - val_loss: 1.6136 - val_categorical_accuracy: 0.1600 - lr: 0.0100\n",
      "Epoch 4/10\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.6474 - categorical_accuracy: 0.1885\n",
      "Epoch 4: saving model to model_init_2022-05-17--18_12_33.848004\\model-00004-1.64738-0.18854-1.60495-0.21000.h5\n",
      "14/14 [==============================] - 62s 5s/step - loss: 1.6474 - categorical_accuracy: 0.1885 - val_loss: 1.6050 - val_categorical_accuracy: 0.2100 - lr: 0.0100\n",
      "Epoch 5/10\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.6485 - categorical_accuracy: 0.2006\n",
      "Epoch 5: saving model to model_init_2022-05-17--18_12_33.848004\\model-00005-1.64851-0.20060-1.61459-0.21000.h5\n",
      "14/14 [==============================] - 63s 5s/step - loss: 1.6485 - categorical_accuracy: 0.2006 - val_loss: 1.6146 - val_categorical_accuracy: 0.2100 - lr: 0.0100\n",
      "Epoch 6/10\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.6374 - categorical_accuracy: 0.2157\n",
      "Epoch 6: saving model to model_init_2022-05-17--18_12_33.848004\\model-00006-1.63736-0.21569-1.61393-0.21000.h5\n",
      "14/14 [==============================] - 62s 5s/step - loss: 1.6374 - categorical_accuracy: 0.2157 - val_loss: 1.6139 - val_categorical_accuracy: 0.2100 - lr: 0.0020\n",
      "Epoch 7/10\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.6457 - categorical_accuracy: 0.1855\n",
      "Epoch 7: saving model to model_init_2022-05-17--18_12_33.848004\\model-00007-1.64570-0.18552-1.62370-0.20000.h5\n",
      "14/14 [==============================] - 62s 5s/step - loss: 1.6457 - categorical_accuracy: 0.1855 - val_loss: 1.6237 - val_categorical_accuracy: 0.2000 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.6507 - categorical_accuracy: 0.2081\n",
      "Epoch 8: saving model to model_init_2022-05-17--18_12_33.848004\\model-00008-1.65070-0.20814-1.62247-0.18000.h5\n",
      "14/14 [==============================] - 63s 5s/step - loss: 1.6507 - categorical_accuracy: 0.2081 - val_loss: 1.6225 - val_categorical_accuracy: 0.1800 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.6505 - categorical_accuracy: 0.2021\n",
      "Epoch 9: saving model to model_init_2022-05-17--18_12_33.848004\\model-00009-1.65053-0.20211-1.60997-0.21000.h5\n",
      "14/14 [==============================] - 62s 5s/step - loss: 1.6505 - categorical_accuracy: 0.2021 - val_loss: 1.6100 - val_categorical_accuracy: 0.2100 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.6495 - categorical_accuracy: 0.1825\n",
      "Epoch 10: saving model to model_init_2022-05-17--18_12_33.848004\\model-00010-1.64953-0.18250-1.61191-0.20000.h5\n",
      "14/14 [==============================] - 62s 5s/step - loss: 1.6495 - categorical_accuracy: 0.1825 - val_loss: 1.6119 - val_categorical_accuracy: 0.2000 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "history=model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=10, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findings: Model 1\n",
    "* Model has very low accuracy.\n",
    "* We will create new model with additional conv2d timedistributed layers\n",
    "- Train loss: 1.6495 \n",
    "- Train categorical_accuracy: 0.1825 \n",
    "- val_loss: 1.6119 \n",
    "- val_categorical_accuracy: 0.2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2\n",
    "* We have included additional conv2d timedistributed layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed_4 (TimeDis  (None, 30, 68, 68, 64)   1792      \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_5 (TimeDis  (None, 30, 68, 68, 64)   0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_6 (TimeDis  (None, 30, 68, 68, 64)   256       \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_7 (TimeDis  (None, 30, 66, 66, 32)   18464     \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_8 (TimeDis  (None, 30, 66, 66, 32)   0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_9 (TimeDis  (None, 30, 66, 66, 32)   128       \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_10 (TimeDi  (None, 30, 33, 33, 32)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_11 (TimeDi  (None, 30, 33, 33, 32)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_12 (TimeDi  (None, 30, 33, 33, 64)   18496     \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_13 (TimeDi  (None, 30, 33, 33, 64)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_14 (TimeDi  (None, 30, 33, 33, 64)   256       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_15 (TimeDi  (None, 30, 31, 31, 64)   36928     \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_16 (TimeDi  (None, 30, 31, 31, 64)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_17 (TimeDi  (None, 30, 31, 31, 64)   256       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_18 (TimeDi  (None, 30, 15, 15, 64)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_19 (TimeDi  (None, 30, 15, 15, 64)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_20 (TimeDi  (None, 30, 14400)        0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 30, 64)            2777472   \n",
      "                                                                 \n",
      " time_distributed_21 (TimeDi  (None, 30, 64)           0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 1920)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 100)               192100    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 5)                 505       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,046,653\n",
      "Trainable params: 3,046,205\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#since Last model is very bad in terms of accuracy, we will add more conv2d timedistributed layers \n",
    "\n",
    "model = Sequential()\n",
    "n_i = 30\n",
    "model.add(TimeDistributed(Conv2D(64,(3,3),activation='relu',input_shape=(n_i, scale_f, scale_f, 3))))\n",
    "model.add(TimeDistributed(Activation('relu')))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(32, (3, 3))))\n",
    "model.add(TimeDistributed(Activation('relu')))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "model.add(TimeDistributed(Dropout(0.25)))\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(64, (3, 3), padding='same')))\n",
    "model.add(TimeDistributed(Activation('relu')))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(Conv2D(64, (3, 3))))\n",
    "model.add(TimeDistributed((Activation('relu'))))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "model.add(TimeDistributed(Dropout(0.25)))\n",
    "\n",
    "\n",
    "# flatten\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "#adding GRU layer with dropout\n",
    "model.add(GRU(64,return_sequences=True))\n",
    "model.add(TimeDistributed(Dropout(0.25)))\n",
    "\n",
    "#flattening again for dense input\n",
    "model.add(Flatten())\n",
    "\n",
    "#taking a single dense before output softmax\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "optimiser = 'sgd'#optimizer\n",
    "\n",
    "input_param = (None, n_i, scale_f,scale_f,3)\n",
    "model.build(input_param)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "#changing batch size to fit in memory\n",
    "batch_size = 30\n",
    "\n",
    "#do for 10 epochs\n",
    "num_epochs = 10\n",
    "train_generator = generator(train_path, train_doc, batch_size,scale_f,n_i)\n",
    "val_generator = generator(val_path, val_doc, batch_size,scale_f,n_i)\n",
    "\n",
    "#save model weights\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','--').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=5, min_lr=0.001) # Reducelronplateau code\n",
    "callbacks_list = [checkpoint, LR]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#steps per epoch\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and Fit the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  train ; batch size = 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\44775\\AppData\\Local\\Temp\\ipykernel_17468\\3178268726.py:3: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.5230 - categorical_accuracy: 0.3605Source path =  val ; batch size = 30\n",
      "\n",
      "Epoch 1: saving model to model_init_2022-05-17--18_12_33.848004\\model-00001-1.52304-0.36048-2.63750-0.18000.h5\n",
      "23/23 [==============================] - 190s 8s/step - loss: 1.5230 - categorical_accuracy: 0.3605 - val_loss: 2.6375 - val_categorical_accuracy: 0.1800 - lr: 0.0100\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.2686 - categorical_accuracy: 0.5189\n",
      "Epoch 2: saving model to model_init_2022-05-17--18_12_33.848004\\model-00002-1.26863-0.51885-4.27341-0.26000.h5\n",
      "23/23 [==============================] - 186s 8s/step - loss: 1.2686 - categorical_accuracy: 0.5189 - val_loss: 4.2734 - val_categorical_accuracy: 0.2600 - lr: 0.0100\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.9509 - categorical_accuracy: 0.6244\n",
      "Epoch 3: saving model to model_init_2022-05-17--18_12_33.848004\\model-00003-0.95087-0.62443-3.66814-0.20000.h5\n",
      "23/23 [==============================] - 187s 8s/step - loss: 0.9509 - categorical_accuracy: 0.6244 - val_loss: 3.6681 - val_categorical_accuracy: 0.2000 - lr: 0.0100\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.5803 - categorical_accuracy: 0.7783\n",
      "Epoch 4: saving model to model_init_2022-05-17--18_12_33.848004\\model-00004-0.58031-0.77828-4.94279-0.22000.h5\n",
      "23/23 [==============================] - 186s 8s/step - loss: 0.5803 - categorical_accuracy: 0.7783 - val_loss: 4.9428 - val_categorical_accuracy: 0.2200 - lr: 0.0100\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.5014 - categorical_accuracy: 0.8145\n",
      "Epoch 5: saving model to model_init_2022-05-17--18_12_33.848004\\model-00005-0.50139-0.81448-2.35999-0.34000.h5\n",
      "23/23 [==============================] - 187s 8s/step - loss: 0.5014 - categorical_accuracy: 0.8145 - val_loss: 2.3600 - val_categorical_accuracy: 0.3400 - lr: 0.0100\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.3741 - categorical_accuracy: 0.8703\n",
      "Epoch 6: saving model to model_init_2022-05-17--18_12_33.848004\\model-00006-0.37407-0.87029-5.21551-0.23000.h5\n",
      "23/23 [==============================] - 188s 8s/step - loss: 0.3741 - categorical_accuracy: 0.8703 - val_loss: 5.2155 - val_categorical_accuracy: 0.2300 - lr: 0.0100\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.3340 - categorical_accuracy: 0.8869\n",
      "Epoch 7: saving model to model_init_2022-05-17--18_12_33.848004\\model-00007-0.33402-0.88688-1.74093-0.49000.h5\n",
      "23/23 [==============================] - 188s 8s/step - loss: 0.3340 - categorical_accuracy: 0.8869 - val_loss: 1.7409 - val_categorical_accuracy: 0.4900 - lr: 0.0100\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.2171 - categorical_accuracy: 0.9276\n",
      "Epoch 8: saving model to model_init_2022-05-17--18_12_33.848004\\model-00008-0.21708-0.92760-2.25046-0.47000.h5\n",
      "23/23 [==============================] - 187s 8s/step - loss: 0.2171 - categorical_accuracy: 0.9276 - val_loss: 2.2505 - val_categorical_accuracy: 0.4700 - lr: 0.0100\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.1872 - categorical_accuracy: 0.9397\n",
      "Epoch 9: saving model to model_init_2022-05-17--18_12_33.848004\\model-00009-0.18723-0.93967-2.60366-0.46000.h5\n",
      "23/23 [==============================] - 190s 8s/step - loss: 0.1872 - categorical_accuracy: 0.9397 - val_loss: 2.6037 - val_categorical_accuracy: 0.4600 - lr: 0.0100\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.2903 - categorical_accuracy: 0.9020\n",
      "Epoch 10: saving model to model_init_2022-05-17--18_12_33.848004\\model-00010-0.29035-0.90196-0.68281-0.75000.h5\n",
      "23/23 [==============================] - 189s 8s/step - loss: 0.2903 - categorical_accuracy: 0.9020 - val_loss: 0.6828 - val_categorical_accuracy: 0.7500 - lr: 0.0100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x248c735cd60>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit model\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findings: Model 2\n",
    "\n",
    "* Model 2 is overfitting. Accuracy on train dataset is 0.9020 where as Accuracy on validation dataset is 0.7500\n",
    "* We will experiment by increasing the size of scaled images and check if the new model can manage the issue of overfitting.\n",
    "* \n",
    "  Total params: 3,046,653\n",
    "  Trainable params: 3,046,205\n",
    "  Non-trainable params: 448\n",
    "  \n",
    "- Train loss: 0.2903 \n",
    "- Train categorical_accuracy: 0.9020 \n",
    "- val_loss: 0.6828 \n",
    "- val_categorical_accuracy: 0.7500 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3\n",
    "* Creating a model with size of the scaled image =70\n",
    "* adding another set of conv2D timedistributed layer for better feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed_22 (TimeDi  (None, 30, 68, 68, 64)   1792      \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_23 (TimeDi  (None, 30, 68, 68, 64)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_24 (TimeDi  (None, 30, 68, 68, 64)   256       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_25 (TimeDi  (None, 30, 66, 66, 32)   18464     \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_26 (TimeDi  (None, 30, 66, 66, 32)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_27 (TimeDi  (None, 30, 66, 66, 32)   128       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_28 (TimeDi  (None, 30, 33, 33, 32)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_29 (TimeDi  (None, 30, 33, 33, 32)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_30 (TimeDi  (None, 30, 33, 33, 64)   18496     \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_31 (TimeDi  (None, 30, 33, 33, 64)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_32 (TimeDi  (None, 30, 33, 33, 64)   256       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_33 (TimeDi  (None, 30, 31, 31, 64)   36928     \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_34 (TimeDi  (None, 30, 31, 31, 64)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_35 (TimeDi  (None, 30, 31, 31, 64)   256       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_36 (TimeDi  (None, 30, 15, 15, 64)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_37 (TimeDi  (None, 30, 15, 15, 64)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_38 (TimeDi  (None, 30, 15, 15, 128)  73856     \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_39 (TimeDi  (None, 30, 15, 15, 128)  0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_40 (TimeDi  (None, 30, 15, 15, 128)  512       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_41 (TimeDi  (None, 30, 13, 13, 128)  147584    \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_42 (TimeDi  (None, 30, 13, 13, 128)  0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_43 (TimeDi  (None, 30, 13, 13, 128)  512       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_44 (TimeDi  (None, 30, 6, 6, 128)    0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_45 (TimeDi  (None, 30, 6, 6, 128)    0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_46 (TimeDi  (None, 30, 4608)         0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " gru_2 (GRU)                 (None, 30, 64)            897408    \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 1920)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 512)               983552    \n",
      "                                                                 \n",
      " activation_10 (Activation)  (None, 512)               0         \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 5)                 2565      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,182,565\n",
      "Trainable params: 2,181,605\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#size of scaled image =70\n",
    "scale_f = 70\n",
    "\n",
    "model = Sequential()\n",
    "n_i = 30\n",
    "model.add(TimeDistributed(Conv2D(64,(3,3),activation='relu',input_shape=(n_i, scale_f, scale_f, 3))))\n",
    "model.add(TimeDistributed(Activation('relu')))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(32, (3, 3))))\n",
    "model.add(TimeDistributed(Activation('relu')))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "model.add(TimeDistributed(Dropout(0.25)))\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(64, (3, 3), padding='same')))\n",
    "model.add(TimeDistributed(Activation('relu')))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(Conv2D(64, (3, 3))))\n",
    "model.add(TimeDistributed((Activation('relu'))))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "model.add(TimeDistributed(Dropout(0.25)))\n",
    "\n",
    "\n",
    "#we have added another set of conv2d timedistributed layers for better feature extraction\n",
    "model.add(TimeDistributed(Conv2D(128, (3, 3), padding='same')))\n",
    "model.add(TimeDistributed(Activation('relu')))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(Conv2D(128, (3, 3))))\n",
    "model.add(TimeDistributed(Activation('relu')))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "model.add(TimeDistributed(Dropout(0.25)))\n",
    "\n",
    "# flatten\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "#adding GRU layers\n",
    "model.add(GRU(64,return_sequences=True))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "#increasing the dense layer size to 512 for better learning\n",
    "model.add(Dense(512,kernel_regularizer=l2(0.01)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "optimiser = 'sgd'#optimizer\n",
    "\n",
    "input_param = (None, n_i, scale_f,scale_f,3)\n",
    "model.build(input_param)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and Fit the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Source path =  train ; batch size = 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\44775\\AppData\\Local\\Temp\\ipykernel_17468\\1776157345.py:34: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "34/34 [==============================] - ETA: 0s - loss: 9.4205 - categorical_accuracy: 0.4072Source path =  val ; batch size = 20\n",
      "\n",
      "Epoch 1: saving model to model_init_2022-05-17--18_12_33.848004\\model-00001-9.42045-0.40724-12.43602-0.23000.h5\n",
      "34/34 [==============================] - 210s 6s/step - loss: 9.4205 - categorical_accuracy: 0.4072 - val_loss: 12.4360 - val_categorical_accuracy: 0.2300 - lr: 0.0100\n",
      "Epoch 2/10\n",
      "34/34 [==============================] - ETA: 0s - loss: 9.0109 - categorical_accuracy: 0.5656\n",
      "Epoch 2: saving model to model_init_2022-05-17--18_12_33.848004\\model-00002-9.01089-0.56561-11.42424-0.23000.h5\n",
      "34/34 [==============================] - 206s 6s/step - loss: 9.0109 - categorical_accuracy: 0.5656 - val_loss: 11.4242 - val_categorical_accuracy: 0.2300 - lr: 0.0100\n",
      "Epoch 3/10\n",
      "34/34 [==============================] - ETA: 0s - loss: 8.5514 - categorical_accuracy: 0.7436\n",
      "Epoch 3: saving model to model_init_2022-05-17--18_12_33.848004\\model-00003-8.55141-0.74359-9.66569-0.38000.h5\n",
      "34/34 [==============================] - 207s 6s/step - loss: 8.5514 - categorical_accuracy: 0.7436 - val_loss: 9.6657 - val_categorical_accuracy: 0.3800 - lr: 0.0100\n",
      "Epoch 4/10\n",
      "34/34 [==============================] - ETA: 0s - loss: 8.2989 - categorical_accuracy: 0.7843\n",
      "Epoch 4: saving model to model_init_2022-05-17--18_12_33.848004\\model-00004-8.29889-0.78431-8.70666-0.66000.h5\n",
      "34/34 [==============================] - 208s 6s/step - loss: 8.2989 - categorical_accuracy: 0.7843 - val_loss: 8.7067 - val_categorical_accuracy: 0.6600 - lr: 0.0100\n",
      "Epoch 5/10\n",
      "34/34 [==============================] - ETA: 0s - loss: 8.0518 - categorical_accuracy: 0.8341\n",
      "Epoch 5: saving model to model_init_2022-05-17--18_12_33.848004\\model-00005-8.05175-0.83409-8.14177-0.82000.h5\n",
      "34/34 [==============================] - 208s 6s/step - loss: 8.0518 - categorical_accuracy: 0.8341 - val_loss: 8.1418 - val_categorical_accuracy: 0.8200 - lr: 0.0100\n",
      "Epoch 6/10\n",
      "34/34 [==============================] - ETA: 0s - loss: 7.8593 - categorical_accuracy: 0.8793\n",
      "Epoch 6: saving model to model_init_2022-05-17--18_12_33.848004\\model-00006-7.85927-0.87934-8.17197-0.72000.h5\n",
      "34/34 [==============================] - 209s 6s/step - loss: 7.8593 - categorical_accuracy: 0.8793 - val_loss: 8.1720 - val_categorical_accuracy: 0.7200 - lr: 0.0100\n",
      "Epoch 7/10\n",
      "34/34 [==============================] - ETA: 0s - loss: 7.6679 - categorical_accuracy: 0.8914\n",
      "Epoch 7: saving model to model_init_2022-05-17--18_12_33.848004\\model-00007-7.66791-0.89140-7.87266-0.83000.h5\n",
      "34/34 [==============================] - 208s 6s/step - loss: 7.6679 - categorical_accuracy: 0.8914 - val_loss: 7.8727 - val_categorical_accuracy: 0.8300 - lr: 0.0100\n",
      "Epoch 8/10\n",
      "34/34 [==============================] - ETA: 0s - loss: 7.5093 - categorical_accuracy: 0.9397\n",
      "Epoch 8: saving model to model_init_2022-05-17--18_12_33.848004\\model-00008-7.50933-0.93967-7.98158-0.79000.h5\n",
      "34/34 [==============================] - 209s 6s/step - loss: 7.5093 - categorical_accuracy: 0.9397 - val_loss: 7.9816 - val_categorical_accuracy: 0.7900 - lr: 0.0100\n",
      "Epoch 9/10\n",
      "34/34 [==============================] - ETA: 0s - loss: 7.3416 - categorical_accuracy: 0.9683\n",
      "Epoch 9: saving model to model_init_2022-05-17--18_12_33.848004\\model-00009-7.34159-0.96833-8.63032-0.50000.h5\n",
      "34/34 [==============================] - 209s 6s/step - loss: 7.3416 - categorical_accuracy: 0.9683 - val_loss: 8.6303 - val_categorical_accuracy: 0.5000 - lr: 0.0100\n",
      "Epoch 10/10\n",
      "34/34 [==============================] - ETA: 0s - loss: 7.3055 - categorical_accuracy: 0.9367\n",
      "Epoch 10: saving model to model_init_2022-05-17--18_12_33.848004\\model-00010-7.30548-0.93665-7.75964-0.76000.h5\n",
      "34/34 [==============================] - 209s 6s/step - loss: 7.3055 - categorical_accuracy: 0.9367 - val_loss: 7.7596 - val_categorical_accuracy: 0.7600 - lr: 0.0100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x248d6b15270>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 20\n",
    "num_epochs = 10\n",
    "train_generator = generator(train_path, train_doc, batch_size,scale_f, n_i)\n",
    "val_generator = generator(val_path, val_doc, batch_size,scale_f, n_i)\n",
    "\n",
    "#save model weights\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','--').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=5, min_lr=0.001) #Reducelronplateau code \n",
    "callbacks_list = [checkpoint, LR]\n",
    "\n",
    "\n",
    "#steps per epoch\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1\n",
    "    \n",
    "#fit model\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings\n",
    "* models in few epochs has manage to sort the overfitting issue but later epoch still shown overfitting. \n",
    "* we will try to reduce the overfitting as well as loss in next model\n",
    "\n",
    "- Train loss: 7.3055 \n",
    "- Train categorical_accuracy: 0.9367 \n",
    "- val_loss: 7.7596 \n",
    "- val_categorical_accuracy: 0.7600\n",
    "\n",
    "* Total params: 2,182,565\n",
    "  Trainable params: 2,181,605\n",
    "  Non-trainable params: 960\n",
    "___________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build, Compile and Fit the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Source path =  train ; batch size = 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\44775\\AppData\\Local\\Temp\\ipykernel_17468\\2170958243.py:35: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 7.1317 - categorical_accuracy: 0.9608Source path =  val ; batch size = 20\n",
      "\n",
      "Epoch 1: saving model to model_init_2022-05-17--18_12_33.848004\\model-00001-7.13168-0.96078-7.74347-0.76000.h5\n",
      "34/34 [==============================] - 211s 6s/step - loss: 7.1317 - categorical_accuracy: 0.9608 - val_loss: 7.7435 - val_categorical_accuracy: 0.7600 - lr: 0.0100\n",
      "Epoch 2/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 6.9974 - categorical_accuracy: 0.9819\n",
      "Epoch 2: saving model to model_init_2022-05-17--18_12_33.848004\\model-00002-6.99736-0.98190-7.61347-0.78000.h5\n",
      "34/34 [==============================] - 208s 6s/step - loss: 6.9974 - categorical_accuracy: 0.9819 - val_loss: 7.6135 - val_categorical_accuracy: 0.7800 - lr: 0.0100\n",
      "Epoch 3/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 6.8795 - categorical_accuracy: 0.9940\n",
      "Epoch 3: saving model to model_init_2022-05-17--18_12_33.848004\\model-00003-6.87951-0.99397-7.66171-0.64000.h5\n",
      "34/34 [==============================] - 209s 6s/step - loss: 6.8795 - categorical_accuracy: 0.9940 - val_loss: 7.6617 - val_categorical_accuracy: 0.6400 - lr: 0.0100\n",
      "Epoch 4/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 6.8332 - categorical_accuracy: 0.9894\n",
      "Epoch 4: saving model to model_init_2022-05-17--18_12_33.848004\\model-00004-6.83324-0.98944-7.41076-0.79000.h5\n",
      "34/34 [==============================] - 209s 6s/step - loss: 6.8332 - categorical_accuracy: 0.9894 - val_loss: 7.4108 - val_categorical_accuracy: 0.7900 - lr: 0.0020\n",
      "Epoch 5/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 6.8033 - categorical_accuracy: 0.9940\n",
      "Epoch 5: saving model to model_init_2022-05-17--18_12_33.848004\\model-00005-6.80329-0.99397-7.39985-0.76000.h5\n",
      "34/34 [==============================] - 209s 6s/step - loss: 6.8033 - categorical_accuracy: 0.9940 - val_loss: 7.3998 - val_categorical_accuracy: 0.7600 - lr: 0.0020\n",
      "Epoch 6/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 6.7752 - categorical_accuracy: 0.9985\n",
      "Epoch 6: saving model to model_init_2022-05-17--18_12_33.848004\\model-00006-6.77517-0.99849-7.28786-0.80000.h5\n",
      "34/34 [==============================] - 210s 6s/step - loss: 6.7752 - categorical_accuracy: 0.9985 - val_loss: 7.2879 - val_categorical_accuracy: 0.8000 - lr: 0.0020\n",
      "Epoch 7/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 6.7661 - categorical_accuracy: 0.9970\n",
      "Epoch 7: saving model to model_init_2022-05-17--18_12_33.848004\\model-00007-6.76609-0.99698-7.18996-0.83000.h5\n",
      "34/34 [==============================] - 210s 6s/step - loss: 6.7661 - categorical_accuracy: 0.9970 - val_loss: 7.1900 - val_categorical_accuracy: 0.8300 - lr: 0.0020\n",
      "Epoch 8/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 6.7366 - categorical_accuracy: 0.9985\n",
      "Epoch 8: saving model to model_init_2022-05-17--18_12_33.848004\\model-00008-6.73655-0.99849-7.33282-0.79000.h5\n",
      "34/34 [==============================] - 211s 6s/step - loss: 6.7366 - categorical_accuracy: 0.9985 - val_loss: 7.3328 - val_categorical_accuracy: 0.7900 - lr: 0.0020\n",
      "Epoch 9/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 6.7243 - categorical_accuracy: 0.9985\n",
      "Epoch 9: saving model to model_init_2022-05-17--18_12_33.848004\\model-00009-6.72428-0.99849-7.19685-0.80000.h5\n",
      "34/34 [==============================] - 210s 6s/step - loss: 6.7243 - categorical_accuracy: 0.9985 - val_loss: 7.1969 - val_categorical_accuracy: 0.8000 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 6.7103 - categorical_accuracy: 0.9985\n",
      "Epoch 10: saving model to model_init_2022-05-17--18_12_33.848004\\model-00010-6.71031-0.99849-7.10908-0.82000.h5\n",
      "34/34 [==============================] - 210s 6s/step - loss: 6.7103 - categorical_accuracy: 0.9985 - val_loss: 7.1091 - val_categorical_accuracy: 0.8200 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 6.7015 - categorical_accuracy: 1.0000\n",
      "Epoch 11: saving model to model_init_2022-05-17--18_12_33.848004\\model-00011-6.70151-1.00000-7.12994-0.83000.h5\n",
      "34/34 [==============================] - 210s 6s/step - loss: 6.7015 - categorical_accuracy: 1.0000 - val_loss: 7.1299 - val_categorical_accuracy: 0.8300 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 6.6915 - categorical_accuracy: 0.9970\n",
      "Epoch 12: saving model to model_init_2022-05-17--18_12_33.848004\\model-00012-6.69147-0.99698-7.15914-0.81000.h5\n",
      "34/34 [==============================] - 210s 6s/step - loss: 6.6915 - categorical_accuracy: 0.9970 - val_loss: 7.1591 - val_categorical_accuracy: 0.8100 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 6.6797 - categorical_accuracy: 1.0000\n",
      "Epoch 13: saving model to model_init_2022-05-17--18_12_33.848004\\model-00013-6.67974-1.00000-7.17150-0.82000.h5\n",
      "34/34 [==============================] - 211s 6s/step - loss: 6.6797 - categorical_accuracy: 1.0000 - val_loss: 7.1715 - val_categorical_accuracy: 0.8200 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 6.6740 - categorical_accuracy: 0.9985\n",
      "Epoch 14: saving model to model_init_2022-05-17--18_12_33.848004\\model-00014-6.67403-0.99849-7.02668-0.85000.h5\n",
      "34/34 [==============================] - 209s 6s/step - loss: 6.6740 - categorical_accuracy: 0.9985 - val_loss: 7.0267 - val_categorical_accuracy: 0.8500 - lr: 0.0010\n",
      "Epoch 15/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 6.6626 - categorical_accuracy: 1.0000\n",
      "Epoch 15: saving model to model_init_2022-05-17--18_12_33.848004\\model-00015-6.66255-1.00000-7.06810-0.84000.h5\n",
      "34/34 [==============================] - 211s 6s/step - loss: 6.6626 - categorical_accuracy: 1.0000 - val_loss: 7.0681 - val_categorical_accuracy: 0.8400 - lr: 0.0010\n",
      "Epoch 16/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 6.6532 - categorical_accuracy: 0.9985\n",
      "Epoch 16: saving model to model_init_2022-05-17--18_12_33.848004\\model-00016-6.65321-0.99849-7.06796-0.84000.h5\n",
      "34/34 [==============================] - 210s 6s/step - loss: 6.6532 - categorical_accuracy: 0.9985 - val_loss: 7.0680 - val_categorical_accuracy: 0.8400 - lr: 0.0010\n",
      "Epoch 17/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 6.6430 - categorical_accuracy: 1.0000\n",
      "Epoch 17: saving model to model_init_2022-05-17--18_12_33.848004\\model-00017-6.64303-1.00000-7.16477-0.82000.h5\n",
      "34/34 [==============================] - 210s 6s/step - loss: 6.6430 - categorical_accuracy: 1.0000 - val_loss: 7.1648 - val_categorical_accuracy: 0.8200 - lr: 0.0010\n",
      "Epoch 18/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 6.6344 - categorical_accuracy: 1.0000\n",
      "Epoch 18: saving model to model_init_2022-05-17--18_12_33.848004\\model-00018-6.63439-1.00000-7.05284-0.82000.h5\n",
      "34/34 [==============================] - 210s 6s/step - loss: 6.6344 - categorical_accuracy: 1.0000 - val_loss: 7.0528 - val_categorical_accuracy: 0.8200 - lr: 0.0010\n",
      "Epoch 19/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 6.6311 - categorical_accuracy: 0.9985\n",
      "Epoch 19: saving model to model_init_2022-05-17--18_12_33.848004\\model-00019-6.63112-0.99849-7.07086-0.83000.h5\n",
      "34/34 [==============================] - 211s 6s/step - loss: 6.6311 - categorical_accuracy: 0.9985 - val_loss: 7.0709 - val_categorical_accuracy: 0.8300 - lr: 0.0010\n",
      "Epoch 20/20\n",
      "34/34 [==============================] - ETA: 0s - loss: 6.6168 - categorical_accuracy: 0.9985\n",
      "Epoch 20: saving model to model_init_2022-05-17--18_12_33.848004\\model-00020-6.61677-0.99849-7.05304-0.83000.h5\n",
      "34/34 [==============================] - 218s 6s/step - loss: 6.6168 - categorical_accuracy: 0.9985 - val_loss: 7.0530 - val_categorical_accuracy: 0.8300 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x248edf93790>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now let's again check with decreasing learning rate at each epoch (patience = 1)\n",
    "batch_size = 20\n",
    "num_epochs = 20\n",
    "train_generator = generator(train_path, train_doc, batch_size,scale_f, n_i)\n",
    "val_generator = generator(val_path, val_doc, batch_size,scale_f, n_i)\n",
    "\n",
    "#save model weights\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','--').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=1, min_lr=0.001) #Reducelronplateau code\n",
    "callbacks_list = [checkpoint, LR]\n",
    "\n",
    "\n",
    "#steps per epoch\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1\n",
    "    \n",
    "#fit model\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings\n",
    "* Model is still overfitting.So we will use transfer learning in the next model\n",
    "- Train loss: 6.6168 \n",
    "- Train categorical_accuracy: 0.9985 \n",
    "- val_loss: 7.0530\n",
    "- val_categorical_accuracy: 0.8300 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 5\n",
    " * We should use Resnet50 for transfer learning\n",
    " * We kept the learning rate at 0.0004"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed_47 (TimeDi  (None, 20, 7, 7, 2048)   23587712  \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_48 (TimeDi  (None, 20, 2048)         0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_49 (TimeDi  (None, 20, 2048)         0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " gru_3 (GRU)                 (None, 20, 1000)          9150000   \n",
      "                                                                 \n",
      " time_distributed_50 (TimeDi  (None, 20, 100)          100100    \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_51 (TimeDi  (None, 20, 100)          0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 2000)              0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 64)                128064    \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 32,966,201\n",
      "Trainable params: 19,368,633\n",
      "Non-trainable params: 13,597,568\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#let's try transfer learning with Resnet\n",
    "scale_f = 197  #size of image \n",
    "n_i = 20\n",
    "\n",
    "# create a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(scale_f, scale_f, 3))\n",
    "\n",
    "#making some layers trainable\n",
    "split_at = 150\n",
    "for layer in base_model.layers[:split_at]: layer.trainable = False\n",
    "for layer in base_model.layers[split_at:]: layer.trainable = True\n",
    "\n",
    "model.add(TimeDistributed(base_model, input_shape=(n_i, scale_f, scale_f, 3)))\n",
    "model.add(TimeDistributed(GlobalAveragePooling2D()))\n",
    "# flatten\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "#adding GRU/LSTM layers\n",
    "model.add(GRU(1000,return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(100, activation='relu')))\n",
    "model.add(TimeDistributed(Dropout(0.30)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "optimiser = 'sgd' #optimizer\n",
    "\n",
    "input_param = (None, n_i, scale_f,scale_f,3)\n",
    "model.build(input_param)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and Fit the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Source path =  train ; batch size = 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\44775\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n",
      "C:\\Users\\44775\\AppData\\Local\\Temp\\ipykernel_17468\\1976619657.py:40: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.6304 - categorical_accuracy: 0.2700 Source path =  val ; batch size = 20\n",
      "\n",
      "Epoch 1: saving model to model_init_2022-05-17--18_12_33.848004\\model-00001-1.63043-0.26998-1.48802-0.39000.h5\n",
      "34/34 [==============================] - 971s 28s/step - loss: 1.6304 - categorical_accuracy: 0.2700 - val_loss: 1.4880 - val_categorical_accuracy: 0.3900 - lr: 4.0000e-04\n",
      "Epoch 2/15\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.2433 - categorical_accuracy: 0.5430 \n",
      "Epoch 2: saving model to model_init_2022-05-17--18_12_33.848004\\model-00002-1.24331-0.54299-0.89172-0.78000.h5\n",
      "34/34 [==============================] - 966s 28s/step - loss: 1.2433 - categorical_accuracy: 0.5430 - val_loss: 0.8917 - val_categorical_accuracy: 0.7800 - lr: 4.0000e-04\n",
      "Epoch 3/15\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.8154 - categorical_accuracy: 0.7179 \n",
      "Epoch 3: saving model to model_init_2022-05-17--18_12_33.848004\\model-00003-0.81538-0.71795-0.54569-0.85000.h5\n",
      "34/34 [==============================] - 961s 28s/step - loss: 0.8154 - categorical_accuracy: 0.7179 - val_loss: 0.5457 - val_categorical_accuracy: 0.8500 - lr: 4.0000e-04\n",
      "Epoch 4/15\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.4929 - categorical_accuracy: 0.8462 \n",
      "Epoch 4: saving model to model_init_2022-05-17--18_12_33.848004\\model-00004-0.49292-0.84615-0.33369-0.90000.h5\n",
      "34/34 [==============================] - 962s 28s/step - loss: 0.4929 - categorical_accuracy: 0.8462 - val_loss: 0.3337 - val_categorical_accuracy: 0.9000 - lr: 4.0000e-04\n",
      "Epoch 5/15\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2990 - categorical_accuracy: 0.9201 \n",
      "Epoch 5: saving model to model_init_2022-05-17--18_12_33.848004\\model-00005-0.29904-0.92006-0.22265-0.95000.h5\n",
      "34/34 [==============================] - 962s 28s/step - loss: 0.2990 - categorical_accuracy: 0.9201 - val_loss: 0.2227 - val_categorical_accuracy: 0.9500 - lr: 4.0000e-04\n",
      "Epoch 6/15\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.1966 - categorical_accuracy: 0.9472 \n",
      "Epoch 6: saving model to model_init_2022-05-17--18_12_33.848004\\model-00006-0.19662-0.94721-0.17677-0.96000.h5\n",
      "34/34 [==============================] - 1029s 30s/step - loss: 0.1966 - categorical_accuracy: 0.9472 - val_loss: 0.1768 - val_categorical_accuracy: 0.9600 - lr: 4.0000e-04\n",
      "Epoch 7/15\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.1262 - categorical_accuracy: 0.9729 \n",
      "Epoch 7: saving model to model_init_2022-05-17--18_12_33.848004\\model-00007-0.12615-0.97285-0.16939-0.95000.h5\n",
      "34/34 [==============================] - 1047s 31s/step - loss: 0.1262 - categorical_accuracy: 0.9729 - val_loss: 0.1694 - val_categorical_accuracy: 0.9500 - lr: 4.0000e-04\n",
      "Epoch 8/15\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0839 - categorical_accuracy: 0.9925 \n",
      "Epoch 8: saving model to model_init_2022-05-17--18_12_33.848004\\model-00008-0.08391-0.99246-0.12377-0.94000.h5\n",
      "34/34 [==============================] - 1030s 30s/step - loss: 0.0839 - categorical_accuracy: 0.9925 - val_loss: 0.1238 - val_categorical_accuracy: 0.9400 - lr: 4.0000e-04\n",
      "Epoch 9/15\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0871 - categorical_accuracy: 0.9849 \n",
      "Epoch 9: saving model to model_init_2022-05-17--18_12_33.848004\\model-00009-0.08708-0.98492-0.14054-0.94000.h5\n",
      "34/34 [==============================] - 1026s 30s/step - loss: 0.0871 - categorical_accuracy: 0.9849 - val_loss: 0.1405 - val_categorical_accuracy: 0.9400 - lr: 4.0000e-04\n",
      "Epoch 10/15\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0583 - categorical_accuracy: 0.9925 \n",
      "Epoch 10: saving model to model_init_2022-05-17--18_12_33.848004\\model-00010-0.05834-0.99246-0.08842-0.97000.h5\n",
      "34/34 [==============================] - 1030s 30s/step - loss: 0.0583 - categorical_accuracy: 0.9925 - val_loss: 0.0884 - val_categorical_accuracy: 0.9700 - lr: 4.0000e-04\n",
      "Epoch 11/15\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0435 - categorical_accuracy: 0.9925 \n",
      "Epoch 11: saving model to model_init_2022-05-17--18_12_33.848004\\model-00011-0.04352-0.99246-0.08611-0.96000.h5\n",
      "34/34 [==============================] - 1036s 31s/step - loss: 0.0435 - categorical_accuracy: 0.9925 - val_loss: 0.0861 - val_categorical_accuracy: 0.9600 - lr: 4.0000e-04\n",
      "Epoch 12/15\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0447 - categorical_accuracy: 0.9879 \n",
      "Epoch 12: saving model to model_init_2022-05-17--18_12_33.848004\\model-00012-0.04473-0.98793-0.07987-0.98000.h5\n",
      "34/34 [==============================] - 1032s 30s/step - loss: 0.0447 - categorical_accuracy: 0.9879 - val_loss: 0.0799 - val_categorical_accuracy: 0.9800 - lr: 4.0000e-04\n",
      "Epoch 13/15\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0346 - categorical_accuracy: 0.9955 \n",
      "Epoch 13: saving model to model_init_2022-05-17--18_12_33.848004\\model-00013-0.03458-0.99548-0.04689-0.99000.h5\n",
      "34/34 [==============================] - 1023s 30s/step - loss: 0.0346 - categorical_accuracy: 0.9955 - val_loss: 0.0469 - val_categorical_accuracy: 0.9900 - lr: 4.0000e-04\n",
      "Epoch 14/15\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0268 - categorical_accuracy: 0.9970 \n",
      "Epoch 14: saving model to model_init_2022-05-17--18_12_33.848004\\model-00014-0.02678-0.99698-0.09068-0.96000.h5\n",
      "34/34 [==============================] - 1030s 30s/step - loss: 0.0268 - categorical_accuracy: 0.9970 - val_loss: 0.0907 - val_categorical_accuracy: 0.9600 - lr: 4.0000e-04\n",
      "Epoch 15/15\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0296 - categorical_accuracy: 0.9985 \n",
      "Epoch 15: saving model to model_init_2022-05-17--18_12_33.848004\\model-00015-0.02959-0.99849-0.10559-0.97000.h5\n",
      "34/34 [==============================] - 1031s 30s/step - loss: 0.0296 - categorical_accuracy: 0.9985 - val_loss: 0.1056 - val_categorical_accuracy: 0.9700 - lr: 4.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x249099fafe0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#increasing epochs and decreasing learning rate to 0.0004\n",
    "\n",
    "batch_size = 20\n",
    "num_epochs = 15\n",
    "train_generator = generator(train_path, train_doc, batch_size,scale_f, n_i)\n",
    "val_generator = generator(val_path, val_doc, batch_size,scale_f, n_i)\n",
    "\n",
    "#save model weights\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','--').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=1, min_lr=0.0004) #Reducelronplateau code\n",
    "callbacks_list = [checkpoint, LR]\n",
    "\n",
    "\n",
    "#steps per epoch\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1\n",
    "    \n",
    "#fit model\n",
    "# Choosing lower learning rate for fine-tuning\n",
    "# learning rate is generally 10-1000 times lower than normal learning rate when we are fine tuning the initial layers\n",
    "sgd = tf.keras.optimizers.SGD(lr=0.0004, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['categorical_accuracy'])\n",
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings\n",
    "* Resnet50 model has managed the overfitting well. Transfer learning has been successful.\n",
    "- Train loss: 0.0296\n",
    "- Train categorical_accuracy: 0.9985\n",
    "- val_loss: 0.1056 \n",
    "- val_categorical_accuracy: 0.9700 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 6\n",
    "\n",
    " * now we should fine tune this model more,  train over 140th layer\n",
    " * we also changing GRU layer to LSTM\n",
    " * We kept the learning rate at 0.0005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed (TimeDistr  (None, 20, 7, 7, 2048)   23587712  \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDis  (None, 20, 2048)         0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_2 (TimeDis  (None, 20, 2048)         0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 20, 1024)          12587008  \n",
      "                                                                 \n",
      " time_distributed_3 (TimeDis  (None, 20, 128)          131200    \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_4 (TimeDis  (None, 20, 128)          0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 2560)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                163904    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 36,470,149\n",
      "Trainable params: 27,860,485\n",
      "Non-trainable params: 8,609,664\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "scale_f = 197 #size of image limitation with Resnet\n",
    "n_i = 20\n",
    "\n",
    "# create a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(scale_f, scale_f, 3))\n",
    "\n",
    "#making some layers trainable\n",
    "split_at = 140\n",
    "for layer in base_model.layers[:split_at]: layer.trainable = False\n",
    "for layer in base_model.layers[split_at:]: layer.trainable = True\n",
    "\n",
    "model.add(TimeDistributed(base_model, input_shape=(n_i, scale_f, scale_f, 3)))\n",
    "model.add(TimeDistributed(GlobalAveragePooling2D()))\n",
    "# flatten\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "#adding GRU/LSTM layers of more units and adding timedistributed dense layer of 128 neurons each\n",
    "model.add(LSTM(1024,return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(128, activation='relu')))\n",
    "model.add(TimeDistributed(Dropout(0.30)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "optimiser = 'sgd' #optimizer\n",
    "\n",
    "input_param = (None, n_i, scale_f,scale_f,3)\n",
    "model.build(input_param)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and Fit the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Source path =  train ; batch size = 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\44775\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n",
      "C:\\Users\\44775\\AppData\\Local\\Temp\\ipykernel_15412\\852158387.py:40: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.6078 - categorical_accuracy: 0.2157 Source path =  val ; batch size = 20\n",
      "\n",
      "Epoch 1: saving model to model_init_2022-05-18--10_56_12.253688\\model-00001-1.60776-0.21569-1.50426-0.41000.h5\n",
      "34/34 [==============================] - 1039s 31s/step - loss: 1.6078 - categorical_accuracy: 0.2157 - val_loss: 1.5043 - val_categorical_accuracy: 0.4100 - lr: 5.0000e-04\n",
      "Epoch 2/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.3588 - categorical_accuracy: 0.4736 \n",
      "Epoch 2: saving model to model_init_2022-05-18--10_56_12.253688\\model-00002-1.35882-0.47360-1.12993-0.69000.h5\n",
      "34/34 [==============================] - 1025s 30s/step - loss: 1.3588 - categorical_accuracy: 0.4736 - val_loss: 1.1299 - val_categorical_accuracy: 0.6900 - lr: 5.0000e-04\n",
      "Epoch 3/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.9258 - categorical_accuracy: 0.7481 \n",
      "Epoch 3: saving model to model_init_2022-05-18--10_56_12.253688\\model-00003-0.92583-0.74811-0.69630-0.76000.h5\n",
      "34/34 [==============================] - 1027s 30s/step - loss: 0.9258 - categorical_accuracy: 0.7481 - val_loss: 0.6963 - val_categorical_accuracy: 0.7600 - lr: 5.0000e-04\n",
      "Epoch 4/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.5146 - categorical_accuracy: 0.8492 \n",
      "Epoch 4: saving model to model_init_2022-05-18--10_56_12.253688\\model-00004-0.51456-0.84917-0.42567-0.84000.h5\n",
      "34/34 [==============================] - 1025s 30s/step - loss: 0.5146 - categorical_accuracy: 0.8492 - val_loss: 0.4257 - val_categorical_accuracy: 0.8400 - lr: 5.0000e-04\n",
      "Epoch 5/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.3134 - categorical_accuracy: 0.9170 \n",
      "Epoch 5: saving model to model_init_2022-05-18--10_56_12.253688\\model-00005-0.31335-0.91704-0.27307-0.93000.h5\n",
      "34/34 [==============================] - 1026s 30s/step - loss: 0.3134 - categorical_accuracy: 0.9170 - val_loss: 0.2731 - val_categorical_accuracy: 0.9300 - lr: 5.0000e-04\n",
      "Epoch 6/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.2442 - categorical_accuracy: 0.9351 \n",
      "Epoch 6: saving model to model_init_2022-05-18--10_56_12.253688\\model-00006-0.24420-0.93514-0.18863-0.98000.h5\n",
      "34/34 [==============================] - 1033s 30s/step - loss: 0.2442 - categorical_accuracy: 0.9351 - val_loss: 0.1886 - val_categorical_accuracy: 0.9800 - lr: 5.0000e-04\n",
      "Epoch 7/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.1267 - categorical_accuracy: 0.9698 \n",
      "Epoch 7: saving model to model_init_2022-05-18--10_56_12.253688\\model-00007-0.12669-0.96983-0.16205-0.97000.h5\n",
      "34/34 [==============================] - 1041s 31s/step - loss: 0.1267 - categorical_accuracy: 0.9698 - val_loss: 0.1621 - val_categorical_accuracy: 0.9700 - lr: 5.0000e-04\n",
      "Epoch 8/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0733 - categorical_accuracy: 0.9894 \n",
      "Epoch 8: saving model to model_init_2022-05-18--10_56_12.253688\\model-00008-0.07326-0.98944-0.15568-0.96000.h5\n",
      "34/34 [==============================] - 1033s 30s/step - loss: 0.0733 - categorical_accuracy: 0.9894 - val_loss: 0.1557 - val_categorical_accuracy: 0.9600 - lr: 5.0000e-04\n",
      "Epoch 9/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0737 - categorical_accuracy: 0.9849 \n",
      "Epoch 9: saving model to model_init_2022-05-18--10_56_12.253688\\model-00009-0.07368-0.98492-0.15787-0.96000.h5\n",
      "34/34 [==============================] - 1040s 31s/step - loss: 0.0737 - categorical_accuracy: 0.9849 - val_loss: 0.1579 - val_categorical_accuracy: 0.9600 - lr: 5.0000e-04\n",
      "Epoch 10/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0467 - categorical_accuracy: 0.9894 \n",
      "Epoch 10: saving model to model_init_2022-05-18--10_56_12.253688\\model-00010-0.04669-0.98944-0.11850-0.98000.h5\n",
      "34/34 [==============================] - 1043s 31s/step - loss: 0.0467 - categorical_accuracy: 0.9894 - val_loss: 0.1185 - val_categorical_accuracy: 0.9800 - lr: 5.0000e-04\n",
      "Epoch 11/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0402 - categorical_accuracy: 0.9925 \n",
      "Epoch 11: saving model to model_init_2022-05-18--10_56_12.253688\\model-00011-0.04022-0.99246-0.18514-0.94000.h5\n",
      "34/34 [==============================] - 1167s 34s/step - loss: 0.0402 - categorical_accuracy: 0.9925 - val_loss: 0.1851 - val_categorical_accuracy: 0.9400 - lr: 5.0000e-04\n",
      "Epoch 12/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0266 - categorical_accuracy: 0.9955 \n",
      "Epoch 12: saving model to model_init_2022-05-18--10_56_12.253688\\model-00012-0.02659-0.99548-0.06832-0.99000.h5\n",
      "34/34 [==============================] - 1207s 35s/step - loss: 0.0266 - categorical_accuracy: 0.9955 - val_loss: 0.0683 - val_categorical_accuracy: 0.9900 - lr: 5.0000e-04\n",
      "Epoch 13/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0159 - categorical_accuracy: 1.0000 \n",
      "Epoch 13: saving model to model_init_2022-05-18--10_56_12.253688\\model-00013-0.01586-1.00000-0.14442-0.97000.h5\n",
      "34/34 [==============================] - 1028s 30s/step - loss: 0.0159 - categorical_accuracy: 1.0000 - val_loss: 0.1444 - val_categorical_accuracy: 0.9700 - lr: 5.0000e-04\n",
      "Epoch 14/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0148 - categorical_accuracy: 1.0000 \n",
      "Epoch 14: saving model to model_init_2022-05-18--10_56_12.253688\\model-00014-0.01483-1.00000-0.05582-0.99000.h5\n",
      "34/34 [==============================] - 1028s 30s/step - loss: 0.0148 - categorical_accuracy: 1.0000 - val_loss: 0.0558 - val_categorical_accuracy: 0.9900 - lr: 5.0000e-04\n",
      "Epoch 15/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0119 - categorical_accuracy: 1.0000 \n",
      "Epoch 15: saving model to model_init_2022-05-18--10_56_12.253688\\model-00015-0.01188-1.00000-0.17022-0.97000.h5\n",
      "34/34 [==============================] - 1024s 30s/step - loss: 0.0119 - categorical_accuracy: 1.0000 - val_loss: 0.1702 - val_categorical_accuracy: 0.9700 - lr: 5.0000e-04\n",
      "Epoch 16/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0163 - categorical_accuracy: 0.9985 \n",
      "Epoch 16: saving model to model_init_2022-05-18--10_56_12.253688\\model-00016-0.01631-0.99849-0.09674-0.98000.h5\n",
      "34/34 [==============================] - 1027s 30s/step - loss: 0.0163 - categorical_accuracy: 0.9985 - val_loss: 0.0967 - val_categorical_accuracy: 0.9800 - lr: 5.0000e-04\n",
      "Epoch 17/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0108 - categorical_accuracy: 0.9970 \n",
      "Epoch 17: saving model to model_init_2022-05-18--10_56_12.253688\\model-00017-0.01076-0.99698-0.09269-0.98000.h5\n",
      "34/34 [==============================] - 1039s 31s/step - loss: 0.0108 - categorical_accuracy: 0.9970 - val_loss: 0.0927 - val_categorical_accuracy: 0.9800 - lr: 5.0000e-04\n",
      "Epoch 18/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0083 - categorical_accuracy: 1.0000 \n",
      "Epoch 18: saving model to model_init_2022-05-18--10_56_12.253688\\model-00018-0.00826-1.00000-0.12501-0.98000.h5\n",
      "34/34 [==============================] - 1311s 39s/step - loss: 0.0083 - categorical_accuracy: 1.0000 - val_loss: 0.1250 - val_categorical_accuracy: 0.9800 - lr: 5.0000e-04\n",
      "Epoch 19/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0109 - categorical_accuracy: 1.0000 \n",
      "Epoch 19: saving model to model_init_2022-05-18--10_56_12.253688\\model-00019-0.01090-1.00000-0.21495-0.93000.h5\n",
      "34/34 [==============================] - 1191s 35s/step - loss: 0.0109 - categorical_accuracy: 1.0000 - val_loss: 0.2150 - val_categorical_accuracy: 0.9300 - lr: 5.0000e-04\n",
      "Epoch 20/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0094 - categorical_accuracy: 1.0000 \n",
      "Epoch 20: saving model to model_init_2022-05-18--10_56_12.253688\\model-00020-0.00936-1.00000-0.11032-0.98000.h5\n",
      "34/34 [==============================] - 1412s 42s/step - loss: 0.0094 - categorical_accuracy: 1.0000 - val_loss: 0.1103 - val_categorical_accuracy: 0.9800 - lr: 5.0000e-04\n",
      "Epoch 21/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0108 - categorical_accuracy: 0.9985 \n",
      "Epoch 21: saving model to model_init_2022-05-18--10_56_12.253688\\model-00021-0.01078-0.99849-0.10685-0.98000.h5\n",
      "34/34 [==============================] - 1028s 30s/step - loss: 0.0108 - categorical_accuracy: 0.9985 - val_loss: 0.1068 - val_categorical_accuracy: 0.9800 - lr: 5.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0057 - categorical_accuracy: 1.0000 \n",
      "Epoch 22: saving model to model_init_2022-05-18--10_56_12.253688\\model-00022-0.00569-1.00000-0.10100-0.98000.h5\n",
      "34/34 [==============================] - 1052s 31s/step - loss: 0.0057 - categorical_accuracy: 1.0000 - val_loss: 0.1010 - val_categorical_accuracy: 0.9800 - lr: 5.0000e-04\n",
      "Epoch 23/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0093 - categorical_accuracy: 0.9985 \n",
      "Epoch 23: saving model to model_init_2022-05-18--10_56_12.253688\\model-00023-0.00929-0.99849-0.13252-0.97000.h5\n",
      "34/34 [==============================] - 1039s 31s/step - loss: 0.0093 - categorical_accuracy: 0.9985 - val_loss: 0.1325 - val_categorical_accuracy: 0.9700 - lr: 5.0000e-04\n",
      "Epoch 24/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0060 - categorical_accuracy: 0.9985 \n",
      "Epoch 24: saving model to model_init_2022-05-18--10_56_12.253688\\model-00024-0.00605-0.99849-0.06907-0.99000.h5\n",
      "34/34 [==============================] - 1182s 35s/step - loss: 0.0060 - categorical_accuracy: 0.9985 - val_loss: 0.0691 - val_categorical_accuracy: 0.9900 - lr: 5.0000e-04\n",
      "Epoch 25/25\n",
      "34/34 [==============================] - ETA: 0s - loss: 0.0051 - categorical_accuracy: 1.0000 \n",
      "Epoch 25: saving model to model_init_2022-05-18--10_56_12.253688\\model-00025-0.00508-1.00000-0.13893-0.96000.h5\n",
      "34/34 [==============================] - 1377s 40s/step - loss: 0.0051 - categorical_accuracy: 1.0000 - val_loss: 0.1389 - val_categorical_accuracy: 0.9600 - lr: 5.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20d2bc24700>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#increasing epochs to 25 and making learning rate to 0.0005\n",
    "\n",
    "batch_size = 20\n",
    "num_epochs = 25\n",
    "train_generator = generator(train_path, train_doc, batch_size,scale_f, n_i)\n",
    "val_generator = generator(val_path, val_doc, batch_size,scale_f, n_i)\n",
    "\n",
    "#save model weights\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','--').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=1, min_lr=0.0005) #Reducelronplateau code\n",
    "callbacks_list = [checkpoint, LR]\n",
    "\n",
    "\n",
    "#steps per epoch\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1\n",
    "    \n",
    "#fit model\n",
    "# Choosing learning rate of 0.0005\n",
    "\n",
    "sgd = tf.keras.optimizers.SGD(lr=0.0005, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['categorical_accuracy'])\n",
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings:\n",
    " * Perfect accuracy on train and valid data. Even Loss has reduced.\n",
    "- Train loss: 0.0060 \n",
    "- Train categorical_accuracy: 0.9985\n",
    "- val_loss: 0.0691\n",
    "- val_categorical_accuracy: 0.9900 \n",
    " *        \n",
    "    Total params: 36,470,149\n",
    "    Trainable params: 27,860,485\n",
    "    Non-trainable params: 8,609,664"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
