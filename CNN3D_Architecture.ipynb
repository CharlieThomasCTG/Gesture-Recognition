{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gesture Recognition\n",
    "In this group project, you are going to build a 3D Conv model that will be able to predict the 5 gestures correctly. Please import the following libraries to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary libraries\n",
    "import numpy as np\n",
    "import os\n",
    "from imageio import imread\n",
    "import cv2\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, LSTM, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation, Dropout, GlobalAveragePooling2D\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D, Conv2D, MaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We set the random seed to reproduce the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "from keras import backend as K\n",
    "from keras.regularizers import l2\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(30)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "config = tf.compat.v1.ConfigProto\n",
    "config.gpu_options\n",
    "config.log_device_placement = True\n",
    "sess = tf.compat.v1.Session(config=config)\n",
    "tf.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We read the folder names for training and validation. We also set the `batch_size` here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc = np.random.permutation(open('train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('val.csv').readlines())\n",
    "batch_size = 15 #experiment with the batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image size is 120 \n",
    "default_y = 120\n",
    "default_z = 120\n",
    "#create a list of image numbers you want to use for a particular video\n",
    "image_idx = list(range(0,30,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for the generator for CNN3D architecture\n",
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = image_idx #create a list of image numbers you want to use for a particular video\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,len(img_idx),default_y,default_z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    image = cv2.resize(image,(default_y,default_z))\n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = (image[:, :, 0] - np.min(image[:, :, 0]))/(np.max(image[:, :, 0])-np.min(image[:, :, 0])) #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = (image[:, :, 1] - np.min(image[:, :, 1]))/(np.max(image[:, :, 1])-np.min(image[:, :, 1])) #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = (image[:, :, 2] - np.min(image[:, :, 2]))/(np.max(image[:, :, 2])-np.min(image[:, :, 2])) #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        \n",
    "        remaining_batch = len(folder_list) - (num_batches * batch_size)      \n",
    "        batch_data = np.zeros((remaining_batch,len(img_idx),default_y,default_z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "        batch_labels = np.zeros((remaining_batch,5))\n",
    "        for folder in range(remaining_batch): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (num_batches*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    image = cv2.resize(image,(default_y,default_z))\n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = (image[:, :, 0] - np.min(image[:, :, 0]))/(np.max(image[:, :, 0])-np.min(image[:, :, 0]))#normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = (image[:, :, 1] - np.min(image[:, :, 1]))/(np.max(image[:, :, 1])-np.min(image[:, :, 1])) #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = (image[:, :, 2] - np.min(image[:, :, 2]))/(np.max(image[:, :, 2])-np.min(image[:, :, 2])) #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*remaining_batch)].strip().split(';')[2])] = 1\n",
    "        yield batch_data, batch_labels                                                              \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here that a video is represented above in the generator as (number of images, height, width, number of channels). Take this into consideration while creating the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 50\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = 'train'\n",
    "val_path = 'val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 50 # choose the number of epochs\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model building process\n",
    "We will make the model using different functionalities that Keras provides. We have used `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. The last layer is the softmax. We will write the model, the next step is to `compile` the model then we print the `summary` of the model and we will  see the total number of parameters you have to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "#Giving a model name\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "\n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "#creating a checkpoint\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4) # REducelronplateau code\n",
    "#Creating a callbacks\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(Conv3D(8, (2, 2, 2), padding='same',\n",
    "                 input_shape=(len(image_idx), default_z, default_y, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv3D(16, (2, 2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model.add(Dropout(0.05))\n",
    "\n",
    "model.add(Conv3D(16, (2, 2, 2), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv3D(32, (2, 2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model.add(Dropout(0.05))\n",
    "\n",
    "model.add(Conv3D(32, (2, 2, 2), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv3D(32, (2, 2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model.add(Dropout(0.05))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#\n",
    "model.add(Dense(5))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- No. Convolution Layers - 6\n",
    "- Convolution Filter Size- (2,2,2)\n",
    "- Pooling filter size - (2,2,2)\n",
    "- Batch Size - 15\n",
    "- Epochs - 10\n",
    "- Image size - (120, 120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the model and print the summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d (Conv3D)             (None, 30, 120, 120, 8)   200       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 30, 120, 120, 8)   0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 30, 120, 120, 8)  32        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv3d_1 (Conv3D)           (None, 29, 119, 119, 16)  1040      \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 29, 119, 119, 16)  0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 29, 119, 119, 16)  64       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling3d (MaxPooling3D  (None, 14, 59, 59, 16)   0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 14, 59, 59, 16)    0         \n",
      "                                                                 \n",
      " conv3d_2 (Conv3D)           (None, 14, 59, 59, 16)    2064      \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 14, 59, 59, 16)    0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 14, 59, 59, 16)   64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv3d_3 (Conv3D)           (None, 13, 58, 58, 32)    4128      \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 13, 58, 58, 32)    0         \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 13, 58, 58, 32)   128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling3d_1 (MaxPooling  (None, 6, 29, 29, 32)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 6, 29, 29, 32)     0         \n",
      "                                                                 \n",
      " conv3d_4 (Conv3D)           (None, 6, 29, 29, 32)     8224      \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 6, 29, 29, 32)     0         \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 6, 29, 29, 32)    128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv3d_5 (Conv3D)           (None, 5, 28, 28, 32)     8224      \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 5, 28, 28, 32)     0         \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 5, 28, 28, 32)    128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling3d_2 (MaxPooling  (None, 2, 14, 14, 32)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 2, 14, 14, 32)     0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 12544)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               3211520   \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               131584    \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 512)               0         \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 2565      \n",
      "                                                                 \n",
      " activation_8 (Activation)   (None, 5)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,370,093\n",
      "Trainable params: 3,369,821\n",
      "Non-trainable params: 272\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser = tf.keras.optimizers.Adam(learning_rate = 0.001) #optimizer with learning rate =0.001\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  train ; batch size = 15\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - ETA: 0s - loss: 4.1510 - categorical_accuracy: 0.3243 Source path =  val ; batch size = 15\n",
      "\n",
      "Epoch 1: saving model to model_init_2022-05-1722_06_33.726637\\model-00001-4.15101-0.32428-2.87815-0.17000.h5\n",
      "45/45 [==============================] - 1397s 31s/step - loss: 4.1510 - categorical_accuracy: 0.3243 - val_loss: 2.8781 - val_categorical_accuracy: 0.1700 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.6100 - categorical_accuracy: 0.3544 \n",
      "Epoch 2: saving model to model_init_2022-05-1722_06_33.726637\\model-00002-1.61003-0.35445-3.40708-0.14000.h5\n",
      "45/45 [==============================] - 1341s 30s/step - loss: 1.6100 - categorical_accuracy: 0.3544 - val_loss: 3.4071 - val_categorical_accuracy: 0.1400 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.4517 - categorical_accuracy: 0.4344 \n",
      "Epoch 3: saving model to model_init_2022-05-1722_06_33.726637\\model-00003-1.45171-0.43439-5.29215-0.16000.h5\n",
      "45/45 [==============================] - 1324s 29s/step - loss: 1.4517 - categorical_accuracy: 0.4344 - val_loss: 5.2922 - val_categorical_accuracy: 0.1600 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.3714 - categorical_accuracy: 0.4585 \n",
      "Epoch 4: saving model to model_init_2022-05-1722_06_33.726637\\model-00004-1.37141-0.45852-5.08423-0.17000.h5\n",
      "45/45 [==============================] - 1349s 30s/step - loss: 1.3714 - categorical_accuracy: 0.4585 - val_loss: 5.0842 - val_categorical_accuracy: 0.1700 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.2439 - categorical_accuracy: 0.5324 \n",
      "Epoch 5: saving model to model_init_2022-05-1722_06_33.726637\\model-00005-1.24391-0.53243-2.93043-0.19000.h5\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "45/45 [==============================] - 1334s 30s/step - loss: 1.2439 - categorical_accuracy: 0.5324 - val_loss: 2.9304 - val_categorical_accuracy: 0.1900 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.0822 - categorical_accuracy: 0.5505 \n",
      "Epoch 6: saving model to model_init_2022-05-1722_06_33.726637\\model-00006-1.08217-0.55053-2.31894-0.21000.h5\n",
      "45/45 [==============================] - 1321s 29s/step - loss: 1.0822 - categorical_accuracy: 0.5505 - val_loss: 2.3189 - val_categorical_accuracy: 0.2100 - lr: 2.0000e-04\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.0113 - categorical_accuracy: 0.6229 \n",
      "Epoch 7: saving model to model_init_2022-05-1722_06_33.726637\\model-00007-1.01134-0.62293-1.97046-0.31000.h5\n",
      "45/45 [==============================] - 1322s 29s/step - loss: 1.0113 - categorical_accuracy: 0.6229 - val_loss: 1.9705 - val_categorical_accuracy: 0.3100 - lr: 2.0000e-04\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8945 - categorical_accuracy: 0.6621 \n",
      "Epoch 8: saving model to model_init_2022-05-1722_06_33.726637\\model-00008-0.89448-0.66214-1.53378-0.44000.h5\n",
      "45/45 [==============================] - 1360s 30s/step - loss: 0.8945 - categorical_accuracy: 0.6621 - val_loss: 1.5338 - val_categorical_accuracy: 0.4400 - lr: 2.0000e-04\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.9389 - categorical_accuracy: 0.6561 \n",
      "Epoch 9: saving model to model_init_2022-05-1722_06_33.726637\\model-00009-0.93887-0.65611-1.22062-0.59000.h5\n",
      "45/45 [==============================] - 1229s 27s/step - loss: 0.9389 - categorical_accuracy: 0.6561 - val_loss: 1.2206 - val_categorical_accuracy: 0.5900 - lr: 2.0000e-04\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8216 - categorical_accuracy: 0.7210 \n",
      "Epoch 10: saving model to model_init_2022-05-1722_06_33.726637\\model-00010-0.82161-0.72097-1.22550-0.61000.h5\n",
      "45/45 [==============================] - 592s 13s/step - loss: 0.8216 - categorical_accuracy: 0.7210 - val_loss: 1.2255 - val_categorical_accuracy: 0.6100 - lr: 2.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20d5f3e48b0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=10, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings\n",
    " - Model is overfitting \n",
    " - Train loss: 0.8216\n",
    " - Train categorical_accuracy: 0.7210 \n",
    " - val_loss: 1.2255 \n",
    " - val_categorical_accuracy: 0.6100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(Conv3D(8, (2, 2, 2), padding='same',\n",
    "                 input_shape=(len(image_idx), default_z, default_y, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv3D(16, (2, 2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Conv3D(16, (2, 2, 2), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv3D(32, (2, 2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Conv3D(32, (2, 2, 2), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv3D(32, (2, 2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model.add(Dense(5))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling and printing the summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_6 (Conv3D)           (None, 30, 120, 120, 8)   200       \n",
      "                                                                 \n",
      " activation_9 (Activation)   (None, 30, 120, 120, 8)   0         \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 30, 120, 120, 8)  32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv3d_7 (Conv3D)           (None, 29, 119, 119, 16)  1040      \n",
      "                                                                 \n",
      " activation_10 (Activation)  (None, 29, 119, 119, 16)  0         \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 29, 119, 119, 16)  64       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling3d_3 (MaxPooling  (None, 14, 59, 59, 16)   0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 14, 59, 59, 16)    0         \n",
      "                                                                 \n",
      " conv3d_8 (Conv3D)           (None, 14, 59, 59, 16)    2064      \n",
      "                                                                 \n",
      " activation_11 (Activation)  (None, 14, 59, 59, 16)    0         \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 14, 59, 59, 16)   64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv3d_9 (Conv3D)           (None, 13, 58, 58, 32)    4128      \n",
      "                                                                 \n",
      " activation_12 (Activation)  (None, 13, 58, 58, 32)    0         \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 13, 58, 58, 32)   128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling3d_4 (MaxPooling  (None, 6, 29, 29, 32)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 6, 29, 29, 32)     0         \n",
      "                                                                 \n",
      " conv3d_10 (Conv3D)          (None, 6, 29, 29, 32)     8224      \n",
      "                                                                 \n",
      " activation_13 (Activation)  (None, 6, 29, 29, 32)     0         \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 6, 29, 29, 32)    128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv3d_11 (Conv3D)          (None, 5, 28, 28, 32)     8224      \n",
      "                                                                 \n",
      " activation_14 (Activation)  (None, 5, 28, 28, 32)     0         \n",
      "                                                                 \n",
      " batch_normalization_11 (Bat  (None, 5, 28, 28, 32)    128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_5 (MaxPooling  (None, 2, 14, 14, 32)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 2, 14, 14, 32)     0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 12544)             0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               3211520   \n",
      "                                                                 \n",
      " activation_15 (Activation)  (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " activation_16 (Activation)  (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 5)                 1285      \n",
      "                                                                 \n",
      " activation_17 (Activation)  (None, 5)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,303,021\n",
      "Trainable params: 3,302,749\n",
      "Non-trainable params: 272\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\44775\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "optimiser = tf.keras.optimizers.Adam(lr = 0.001) #optimizer with learning rate =0.001\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "45/45 [==============================] - ETA: 0s - loss: 5.5786 - categorical_accuracy: 0.3047 \n",
      "Epoch 1: saving model to model_init_2022-05-1722_06_33.726637\\model-00001-5.57865-0.30468-2.41830-0.15000.h5\n",
      "45/45 [==============================] - 532s 12s/step - loss: 5.5786 - categorical_accuracy: 0.3047 - val_loss: 2.4183 - val_categorical_accuracy: 0.1500 - lr: 0.0010\n",
      "Epoch 2/25\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.7630 - categorical_accuracy: 0.3228 \n",
      "Epoch 2: saving model to model_init_2022-05-1722_06_33.726637\\model-00002-1.76303-0.32278-3.66773-0.19000.h5\n",
      "45/45 [==============================] - 533s 12s/step - loss: 1.7630 - categorical_accuracy: 0.3228 - val_loss: 3.6677 - val_categorical_accuracy: 0.1900 - lr: 0.0010\n",
      "Epoch 3/25\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.6586 - categorical_accuracy: 0.3997 \n",
      "Epoch 3: saving model to model_init_2022-05-1722_06_33.726637\\model-00003-1.65862-0.39970-5.04787-0.25000.h5\n",
      "45/45 [==============================] - 531s 12s/step - loss: 1.6586 - categorical_accuracy: 0.3997 - val_loss: 5.0479 - val_categorical_accuracy: 0.2500 - lr: 0.0010\n",
      "Epoch 4/25\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.4276 - categorical_accuracy: 0.4661 \n",
      "Epoch 4: saving model to model_init_2022-05-1722_06_33.726637\\model-00004-1.42764-0.46606-7.30910-0.20000.h5\n",
      "45/45 [==============================] - 531s 12s/step - loss: 1.4276 - categorical_accuracy: 0.4661 - val_loss: 7.3091 - val_categorical_accuracy: 0.2000 - lr: 0.0010\n",
      "Epoch 5/25\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.3612 - categorical_accuracy: 0.4555 \n",
      "Epoch 5: saving model to model_init_2022-05-1722_06_33.726637\\model-00005-1.36116-0.45551-6.71030-0.18000.h5\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "45/45 [==============================] - 532s 12s/step - loss: 1.3612 - categorical_accuracy: 0.4555 - val_loss: 6.7103 - val_categorical_accuracy: 0.1800 - lr: 0.0010\n",
      "Epoch 6/25\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.2889 - categorical_accuracy: 0.4932 \n",
      "Epoch 6: saving model to model_init_2022-05-1722_06_33.726637\\model-00006-1.28889-0.49321-4.38518-0.26000.h5\n",
      "45/45 [==============================] - 531s 12s/step - loss: 1.2889 - categorical_accuracy: 0.4932 - val_loss: 4.3852 - val_categorical_accuracy: 0.2600 - lr: 2.0000e-04\n",
      "Epoch 7/25\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.0957 - categorical_accuracy: 0.5551 \n",
      "Epoch 7: saving model to model_init_2022-05-1722_06_33.726637\\model-00007-1.09574-0.55505-4.48774-0.24000.h5\n",
      "45/45 [==============================] - 534s 12s/step - loss: 1.0957 - categorical_accuracy: 0.5551 - val_loss: 4.4877 - val_categorical_accuracy: 0.2400 - lr: 2.0000e-04\n",
      "Epoch 8/25\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.0365 - categorical_accuracy: 0.5988 \n",
      "Epoch 8: saving model to model_init_2022-05-1722_06_33.726637\\model-00008-1.03648-0.59879-3.52971-0.18000.h5\n",
      "45/45 [==============================] - 533s 12s/step - loss: 1.0365 - categorical_accuracy: 0.5988 - val_loss: 3.5297 - val_categorical_accuracy: 0.1800 - lr: 2.0000e-04\n",
      "Epoch 9/25\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.9665 - categorical_accuracy: 0.6078 \n",
      "Epoch 9: saving model to model_init_2022-05-1722_06_33.726637\\model-00009-0.96650-0.60784-2.40921-0.34000.h5\n",
      "45/45 [==============================] - 531s 12s/step - loss: 0.9665 - categorical_accuracy: 0.6078 - val_loss: 2.4092 - val_categorical_accuracy: 0.3400 - lr: 2.0000e-04\n",
      "Epoch 10/25\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.9980 - categorical_accuracy: 0.6259 \n",
      "Epoch 10: saving model to model_init_2022-05-1722_06_33.726637\\model-00010-0.99798-0.62594-2.03387-0.37000.h5\n",
      "45/45 [==============================] - 530s 12s/step - loss: 0.9980 - categorical_accuracy: 0.6259 - val_loss: 2.0339 - val_categorical_accuracy: 0.3700 - lr: 2.0000e-04\n",
      "Epoch 11/25\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.9460 - categorical_accuracy: 0.6305 \n",
      "Epoch 11: saving model to model_init_2022-05-1722_06_33.726637\\model-00011-0.94598-0.63047-1.41905-0.52000.h5\n",
      "45/45 [==============================] - 530s 12s/step - loss: 0.9460 - categorical_accuracy: 0.6305 - val_loss: 1.4190 - val_categorical_accuracy: 0.5200 - lr: 2.0000e-04\n",
      "Epoch 12/25\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8753 - categorical_accuracy: 0.6712 \n",
      "Epoch 12: saving model to model_init_2022-05-1722_06_33.726637\\model-00012-0.87530-0.67119-1.26932-0.62000.h5\n",
      "45/45 [==============================] - 530s 12s/step - loss: 0.8753 - categorical_accuracy: 0.6712 - val_loss: 1.2693 - val_categorical_accuracy: 0.6200 - lr: 2.0000e-04\n",
      "Epoch 13/25\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.9003 - categorical_accuracy: 0.6606 \n",
      "Epoch 13: saving model to model_init_2022-05-1722_06_33.726637\\model-00013-0.90031-0.66063-1.15440-0.66000.h5\n",
      "45/45 [==============================] - 530s 12s/step - loss: 0.9003 - categorical_accuracy: 0.6606 - val_loss: 1.1544 - val_categorical_accuracy: 0.6600 - lr: 2.0000e-04\n",
      "Epoch 14/25\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.7313 - categorical_accuracy: 0.7134 \n",
      "Epoch 14: saving model to model_init_2022-05-1722_06_33.726637\\model-00014-0.73129-0.71342-1.03227-0.66000.h5\n",
      "45/45 [==============================] - 530s 12s/step - loss: 0.7313 - categorical_accuracy: 0.7134 - val_loss: 1.0323 - val_categorical_accuracy: 0.6600 - lr: 2.0000e-04\n",
      "Epoch 15/25\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8019 - categorical_accuracy: 0.7014 \n",
      "Epoch 15: saving model to model_init_2022-05-1722_06_33.726637\\model-00015-0.80190-0.70136-1.07512-0.71000.h5\n",
      "45/45 [==============================] - 530s 12s/step - loss: 0.8019 - categorical_accuracy: 0.7014 - val_loss: 1.0751 - val_categorical_accuracy: 0.7100 - lr: 2.0000e-04\n",
      "Epoch 16/25\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.7823 - categorical_accuracy: 0.7330 \n",
      "Epoch 16: saving model to model_init_2022-05-1722_06_33.726637\\model-00016-0.78230-0.73303-1.17126-0.60000.h5\n",
      "45/45 [==============================] - 529s 12s/step - loss: 0.7823 - categorical_accuracy: 0.7330 - val_loss: 1.1713 - val_categorical_accuracy: 0.6000 - lr: 2.0000e-04\n",
      "Epoch 17/25\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.7305 - categorical_accuracy: 0.7195 \n",
      "Epoch 17: saving model to model_init_2022-05-1722_06_33.726637\\model-00017-0.73052-0.71946-1.30152-0.65000.h5\n",
      "45/45 [==============================] - 530s 12s/step - loss: 0.7305 - categorical_accuracy: 0.7195 - val_loss: 1.3015 - val_categorical_accuracy: 0.6500 - lr: 2.0000e-04\n",
      "Epoch 18/25\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.6985 - categorical_accuracy: 0.7149 \n",
      "Epoch 18: saving model to model_init_2022-05-1722_06_33.726637\\model-00018-0.69854-0.71493-1.30899-0.67000.h5\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "45/45 [==============================] - 530s 12s/step - loss: 0.6985 - categorical_accuracy: 0.7149 - val_loss: 1.3090 - val_categorical_accuracy: 0.6700 - lr: 2.0000e-04\n",
      "Epoch 19/25\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.6639 - categorical_accuracy: 0.7436 \n",
      "Epoch 19: saving model to model_init_2022-05-1722_06_33.726637\\model-00019-0.66386-0.74359-0.95296-0.69000.h5\n",
      "45/45 [==============================] - 529s 12s/step - loss: 0.6639 - categorical_accuracy: 0.7436 - val_loss: 0.9530 - val_categorical_accuracy: 0.6900 - lr: 4.0000e-05\n",
      "Epoch 20/25\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.6008 - categorical_accuracy: 0.7798 \n",
      "Epoch 20: saving model to model_init_2022-05-1722_06_33.726637\\model-00020-0.60082-0.77979-1.53288-0.66000.h5\n",
      "45/45 [==============================] - 529s 12s/step - loss: 0.6008 - categorical_accuracy: 0.7798 - val_loss: 1.5329 - val_categorical_accuracy: 0.6600 - lr: 4.0000e-05\n",
      "Epoch 21/25\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.6459 - categorical_accuracy: 0.7526 \n",
      "Epoch 21: saving model to model_init_2022-05-1722_06_33.726637\\model-00021-0.64587-0.75264-1.31160-0.71000.h5\n",
      "45/45 [==============================] - 529s 12s/step - loss: 0.6459 - categorical_accuracy: 0.7526 - val_loss: 1.3116 - val_categorical_accuracy: 0.7100 - lr: 4.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/25\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.6026 - categorical_accuracy: 0.7722 \n",
      "Epoch 22: saving model to model_init_2022-05-1722_06_33.726637\\model-00022-0.60255-0.77225-1.00933-0.69000.h5\n",
      "45/45 [==============================] - 528s 12s/step - loss: 0.6026 - categorical_accuracy: 0.7722 - val_loss: 1.0093 - val_categorical_accuracy: 0.6900 - lr: 4.0000e-05\n",
      "Epoch 23/25\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.6032 - categorical_accuracy: 0.7662 \n",
      "Epoch 23: saving model to model_init_2022-05-1722_06_33.726637\\model-00023-0.60317-0.76621-1.15404-0.70000.h5\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "45/45 [==============================] - 529s 12s/step - loss: 0.6032 - categorical_accuracy: 0.7662 - val_loss: 1.1540 - val_categorical_accuracy: 0.7000 - lr: 4.0000e-05\n",
      "Epoch 24/25\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.6380 - categorical_accuracy: 0.7647 \n",
      "Epoch 24: saving model to model_init_2022-05-1722_06_33.726637\\model-00024-0.63801-0.76471-1.12212-0.73000.h5\n",
      "45/45 [==============================] - 529s 12s/step - loss: 0.6380 - categorical_accuracy: 0.7647 - val_loss: 1.1221 - val_categorical_accuracy: 0.7300 - lr: 8.0000e-06\n",
      "Epoch 25/25\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.5430 - categorical_accuracy: 0.7768 \n",
      "Epoch 25: saving model to model_init_2022-05-1722_06_33.726637\\model-00025-0.54301-0.77677-1.13378-0.69000.h5\n",
      "45/45 [==============================] - 529s 12s/step - loss: 0.5430 - categorical_accuracy: 0.7768 - val_loss: 1.1338 - val_categorical_accuracy: 0.6900 - lr: 8.0000e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20d66d43be0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=25, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings\n",
    "- Model is overfitting\n",
    "- Train loss: 0.5430\n",
    "- Train categorical_accuracy: 0.7768\n",
    "- val_loss: 1.1338\n",
    "- val_categorical_accuracy: 0.6900"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(Conv3D(8, (2, 2, 2), padding='same',\n",
    "                 input_shape=(len(image_idx), default_z, default_y, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv3D(16, (2, 2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "#Remove drop out \n",
    "#model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Conv3D(16, (2, 2, 2), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv3D(32, (2, 2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Conv3D(32, (2, 2, 2), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv3D(32, (3, 3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(256, activation = 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model.add(Dense(5))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the model and print the summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_12 (Conv3D)          (None, 30, 120, 120, 8)   200       \n",
      "                                                                 \n",
      " activation_18 (Activation)  (None, 30, 120, 120, 8)   0         \n",
      "                                                                 \n",
      " batch_normalization_12 (Bat  (None, 30, 120, 120, 8)  32        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv3d_13 (Conv3D)          (None, 29, 119, 119, 16)  1040      \n",
      "                                                                 \n",
      " activation_19 (Activation)  (None, 29, 119, 119, 16)  0         \n",
      "                                                                 \n",
      " batch_normalization_13 (Bat  (None, 29, 119, 119, 16)  64       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_6 (MaxPooling  (None, 14, 59, 59, 16)   0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_14 (Conv3D)          (None, 14, 59, 59, 16)    2064      \n",
      "                                                                 \n",
      " activation_20 (Activation)  (None, 14, 59, 59, 16)    0         \n",
      "                                                                 \n",
      " batch_normalization_14 (Bat  (None, 14, 59, 59, 16)   64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv3d_15 (Conv3D)          (None, 13, 58, 58, 32)    4128      \n",
      "                                                                 \n",
      " activation_21 (Activation)  (None, 13, 58, 58, 32)    0         \n",
      "                                                                 \n",
      " batch_normalization_15 (Bat  (None, 13, 58, 58, 32)   128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_7 (MaxPooling  (None, 6, 29, 29, 32)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 6, 29, 29, 32)     0         \n",
      "                                                                 \n",
      " conv3d_16 (Conv3D)          (None, 6, 29, 29, 32)     8224      \n",
      "                                                                 \n",
      " activation_22 (Activation)  (None, 6, 29, 29, 32)     0         \n",
      "                                                                 \n",
      " batch_normalization_16 (Bat  (None, 6, 29, 29, 32)    128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv3d_17 (Conv3D)          (None, 4, 27, 27, 32)     27680     \n",
      "                                                                 \n",
      " activation_23 (Activation)  (None, 4, 27, 27, 32)     0         \n",
      "                                                                 \n",
      " batch_normalization_17 (Bat  (None, 4, 27, 27, 32)    128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling3d_8 (MaxPooling  (None, 2, 13, 13, 32)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 2, 13, 13, 32)     0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 10816)             0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 128)               1384576   \n",
      "                                                                 \n",
      " batch_normalization_18 (Bat  (None, 128)              512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               33024     \n",
      "                                                                 \n",
      " batch_normalization_19 (Bat  (None, 256)              1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 5)                 1285      \n",
      "                                                                 \n",
      " activation_24 (Activation)  (None, 5)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,464,301\n",
      "Trainable params: 1,463,261\n",
      "Non-trainable params: 1,040\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser = tf.keras.optimizers.Adam(lr = 0.001) # optimizer with learning rate = 0.001\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.9987 - categorical_accuracy: 0.3605 \n",
      "Epoch 1: saving model to model_init_2022-05-1722_06_33.726637\\model-00001-1.99866-0.36048-3.39811-0.21000.h5\n",
      "45/45 [==============================] - 528s 12s/step - loss: 1.9987 - categorical_accuracy: 0.3605 - val_loss: 3.3981 - val_categorical_accuracy: 0.2100 - lr: 0.0010\n",
      "Epoch 2/25\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.3532 - categorical_accuracy: 0.5173 \n",
      "Epoch 2: saving model to model_init_2022-05-1722_06_33.726637\\model-00002-1.35324-0.51735-4.13083-0.19000.h5\n",
      "45/45 [==============================] - 527s 12s/step - loss: 1.3532 - categorical_accuracy: 0.5173 - val_loss: 4.1308 - val_categorical_accuracy: 0.1900 - lr: 0.0010\n",
      "Epoch 3/25\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.2123 - categorical_accuracy: 0.5686 \n",
      "Epoch 3: saving model to model_init_2022-05-1722_06_33.726637\\model-00003-1.21232-0.56863-5.00640-0.17000.h5\n",
      "45/45 [==============================] - 527s 12s/step - loss: 1.2123 - categorical_accuracy: 0.5686 - val_loss: 5.0064 - val_categorical_accuracy: 0.1700 - lr: 0.0010\n",
      "Epoch 4/25\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.0984 - categorical_accuracy: 0.6018 \n",
      "Epoch 4: saving model to model_init_2022-05-1722_06_33.726637\\model-00004-1.09838-0.60181-3.50016-0.25000.h5\n",
      "45/45 [==============================] - 526s 12s/step - loss: 1.0984 - categorical_accuracy: 0.6018 - val_loss: 3.5002 - val_categorical_accuracy: 0.2500 - lr: 0.0010\n",
      "Epoch 5/25\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.9188 - categorical_accuracy: 0.6893 \n",
      "Epoch 5: saving model to model_init_2022-05-1722_06_33.726637\\model-00005-0.91877-0.68929-5.41386-0.18000.h5\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "45/45 [==============================] - 527s 12s/step - loss: 0.9188 - categorical_accuracy: 0.6893 - val_loss: 5.4139 - val_categorical_accuracy: 0.1800 - lr: 0.0010\n",
      "Epoch 6/25\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.0480 - categorical_accuracy: 0.6395 \n",
      "Epoch 6: saving model to model_init_2022-05-1722_06_33.726637\\model-00006-1.04799-0.63952-4.14220-0.27000.h5\n",
      "45/45 [==============================] - 527s 12s/step - loss: 1.0480 - categorical_accuracy: 0.6395 - val_loss: 4.1422 - val_categorical_accuracy: 0.2700 - lr: 2.0000e-04\n",
      "Epoch 7/25\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.7066 - categorical_accuracy: 0.7391 \n",
      "Epoch 7: saving model to model_init_2022-05-1722_06_33.726637\\model-00007-0.70664-0.73906-3.13698-0.27000.h5\n",
      "45/45 [==============================] - 527s 12s/step - loss: 0.7066 - categorical_accuracy: 0.7391 - val_loss: 3.1370 - val_categorical_accuracy: 0.2700 - lr: 2.0000e-04\n",
      "Epoch 8/25\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.6361 - categorical_accuracy: 0.7647 \n",
      "Epoch 8: saving model to model_init_2022-05-1722_06_33.726637\\model-00008-0.63607-0.76471-3.00619-0.30000.h5\n",
      "45/45 [==============================] - 527s 12s/step - loss: 0.6361 - categorical_accuracy: 0.7647 - val_loss: 3.0062 - val_categorical_accuracy: 0.3000 - lr: 2.0000e-04\n",
      "Epoch 9/25\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.5910 - categorical_accuracy: 0.7828 \n",
      "Epoch 9: saving model to model_init_2022-05-1722_06_33.726637\\model-00009-0.59100-0.78281-2.41511-0.34000.h5\n",
      "45/45 [==============================] - 527s 12s/step - loss: 0.5910 - categorical_accuracy: 0.7828 - val_loss: 2.4151 - val_categorical_accuracy: 0.3400 - lr: 2.0000e-04\n",
      "Epoch 10/25\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.5886 - categorical_accuracy: 0.7843 \n",
      "Epoch 10: saving model to model_init_2022-05-1722_06_33.726637\\model-00010-0.58863-0.78431-2.12406-0.43000.h5\n",
      "45/45 [==============================] - 527s 12s/step - loss: 0.5886 - categorical_accuracy: 0.7843 - val_loss: 2.1241 - val_categorical_accuracy: 0.4300 - lr: 2.0000e-04\n",
      "Epoch 11/25\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.4908 - categorical_accuracy: 0.8235 \n",
      "Epoch 11: saving model to model_init_2022-05-1722_06_33.726637\\model-00011-0.49076-0.82353-1.50786-0.59000.h5\n",
      "45/45 [==============================] - 526s 12s/step - loss: 0.4908 - categorical_accuracy: 0.8235 - val_loss: 1.5079 - val_categorical_accuracy: 0.5900 - lr: 2.0000e-04\n",
      "Epoch 12/25\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.4549 - categorical_accuracy: 0.8356 \n",
      "Epoch 12: saving model to model_init_2022-05-1722_06_33.726637\\model-00012-0.45490-0.83560-1.27547-0.62000.h5\n",
      "45/45 [==============================] - 526s 12s/step - loss: 0.4549 - categorical_accuracy: 0.8356 - val_loss: 1.2755 - val_categorical_accuracy: 0.6200 - lr: 2.0000e-04\n",
      "Epoch 13/25\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.4669 - categorical_accuracy: 0.8462 \n",
      "Epoch 13: saving model to model_init_2022-05-1722_06_33.726637\\model-00013-0.46692-0.84615-1.26151-0.65000.h5\n",
      "45/45 [==============================] - 527s 12s/step - loss: 0.4669 - categorical_accuracy: 0.8462 - val_loss: 1.2615 - val_categorical_accuracy: 0.6500 - lr: 2.0000e-04\n",
      "Epoch 14/25\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.4852 - categorical_accuracy: 0.8446 \n",
      "Epoch 14: saving model to model_init_2022-05-1722_06_33.726637\\model-00014-0.48518-0.84465-1.05856-0.71000.h5\n",
      "45/45 [==============================] - 527s 12s/step - loss: 0.4852 - categorical_accuracy: 0.8446 - val_loss: 1.0586 - val_categorical_accuracy: 0.7100 - lr: 2.0000e-04\n",
      "Epoch 15/25\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.4023 - categorical_accuracy: 0.8537 \n",
      "Epoch 15: saving model to model_init_2022-05-1722_06_33.726637\\model-00015-0.40233-0.85370-0.97143-0.73000.h5\n",
      "45/45 [==============================] - 528s 12s/step - loss: 0.4023 - categorical_accuracy: 0.8537 - val_loss: 0.9714 - val_categorical_accuracy: 0.7300 - lr: 2.0000e-04\n",
      "Epoch 16/25\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.3752 - categorical_accuracy: 0.8658 \n",
      "Epoch 16: saving model to model_init_2022-05-1722_06_33.726637\\model-00016-0.37523-0.86576-1.16490-0.72000.h5\n",
      "45/45 [==============================] - 527s 12s/step - loss: 0.3752 - categorical_accuracy: 0.8658 - val_loss: 1.1649 - val_categorical_accuracy: 0.7200 - lr: 2.0000e-04\n",
      "Epoch 17/25\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.4252 - categorical_accuracy: 0.8477 \n",
      "Epoch 17: saving model to model_init_2022-05-1722_06_33.726637\\model-00017-0.42523-0.84766-0.87984-0.74000.h5\n",
      "45/45 [==============================] - 529s 12s/step - loss: 0.4252 - categorical_accuracy: 0.8477 - val_loss: 0.8798 - val_categorical_accuracy: 0.7400 - lr: 2.0000e-04\n",
      "Epoch 18/25\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.3839 - categorical_accuracy: 0.8733 \n",
      "Epoch 18: saving model to model_init_2022-05-1722_06_33.726637\\model-00018-0.38390-0.87330-0.96226-0.73000.h5\n",
      "45/45 [==============================] - 529s 12s/step - loss: 0.3839 - categorical_accuracy: 0.8733 - val_loss: 0.9623 - val_categorical_accuracy: 0.7300 - lr: 2.0000e-04\n",
      "Epoch 19/25\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.3487 - categorical_accuracy: 0.8763 \n",
      "Epoch 19: saving model to model_init_2022-05-1722_06_33.726637\\model-00019-0.34875-0.87632-0.84047-0.78000.h5\n",
      "45/45 [==============================] - 529s 12s/step - loss: 0.3487 - categorical_accuracy: 0.8763 - val_loss: 0.8405 - val_categorical_accuracy: 0.7800 - lr: 2.0000e-04\n",
      "Epoch 20/25\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.2942 - categorical_accuracy: 0.8914 \n",
      "Epoch 20: saving model to model_init_2022-05-1722_06_33.726637\\model-00020-0.29418-0.89140-1.28844-0.70000.h5\n",
      "45/45 [==============================] - 529s 12s/step - loss: 0.2942 - categorical_accuracy: 0.8914 - val_loss: 1.2884 - val_categorical_accuracy: 0.7000 - lr: 2.0000e-04\n",
      "Epoch 21/25\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.2783 - categorical_accuracy: 0.9020 \n",
      "Epoch 21: saving model to model_init_2022-05-1722_06_33.726637\\model-00021-0.27830-0.90196-0.84941-0.80000.h5\n",
      "45/45 [==============================] - 528s 12s/step - loss: 0.2783 - categorical_accuracy: 0.9020 - val_loss: 0.8494 - val_categorical_accuracy: 0.8000 - lr: 2.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/25\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.2764 - categorical_accuracy: 0.9170 \n",
      "Epoch 22: saving model to model_init_2022-05-1722_06_33.726637\\model-00022-0.27642-0.91704-0.82312-0.80000.h5\n",
      "45/45 [==============================] - 529s 12s/step - loss: 0.2764 - categorical_accuracy: 0.9170 - val_loss: 0.8231 - val_categorical_accuracy: 0.8000 - lr: 2.0000e-04\n",
      "Epoch 23/25\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.3163 - categorical_accuracy: 0.8944 \n",
      "Epoch 23: saving model to model_init_2022-05-1722_06_33.726637\\model-00023-0.31632-0.89442-1.32134-0.73000.h5\n",
      "45/45 [==============================] - 532s 12s/step - loss: 0.3163 - categorical_accuracy: 0.8944 - val_loss: 1.3213 - val_categorical_accuracy: 0.7300 - lr: 2.0000e-04\n",
      "Epoch 24/25\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.3363 - categorical_accuracy: 0.8839 \n",
      "Epoch 24: saving model to model_init_2022-05-1722_06_33.726637\\model-00024-0.33631-0.88386-1.07712-0.78000.h5\n",
      "45/45 [==============================] - 532s 12s/step - loss: 0.3363 - categorical_accuracy: 0.8839 - val_loss: 1.0771 - val_categorical_accuracy: 0.7800 - lr: 2.0000e-04\n",
      "Epoch 25/25\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.3063 - categorical_accuracy: 0.9005 \n",
      "Epoch 25: saving model to model_init_2022-05-1722_06_33.726637\\model-00025-0.30632-0.90045-0.84883-0.82000.h5\n",
      "45/45 [==============================] - 529s 12s/step - loss: 0.3063 - categorical_accuracy: 0.9005 - val_loss: 0.8488 - val_categorical_accuracy: 0.8200 - lr: 2.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20d5bb2ada0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=25, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings\n",
    "\n",
    "- Overfitting was reduced in the model by removing dropout layer\n",
    "- Train loss: 0.3063\n",
    "- Train categorical_accuracy: 0.9005\n",
    "- val_loss: 0.8488\n",
    "- val_categorical_accuracy: 0.8200"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
