{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gesture Recognition\n",
    "In this group project, you are going to build a 3D Conv model that will be able to predict the 5 gestures correctly. Please import the following libraries to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary libraries\n",
    "import numpy as np\n",
    "import os\n",
    "from imageio import imread\n",
    "import cv2\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, LSTM, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation, Dropout, GlobalAveragePooling2D\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D, Conv2D, MaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We set the random seed to reproduce the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "from keras import backend as K\n",
    "from keras.regularizers import l2\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(30)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "config = tf.compat.v1.ConfigProto\n",
    "config.gpu_options\n",
    "config.log_device_placement = True\n",
    "sess = tf.compat.v1.Session(config=config)\n",
    "tf.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We read the folder names for training and validation. We also set the `batch_size` here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc = np.random.permutation(open('train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('val.csv').readlines())\n",
    "batch_size = 15 #experiment with the batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image size is 120 \n",
    "default_y = 120\n",
    "default_z = 120\n",
    "#create a list of image numbers you want to use for a particular video\n",
    "image_idx = list(range(0,30,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for the generator for CNN3D architecture\n",
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = image_idx #create a list of image numbers you want to use for a particular video\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,len(img_idx),default_y,default_z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    image = cv2.resize(image,(default_y,default_z))\n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = (image[:, :, 0] - np.min(image[:, :, 0]))/(np.max(image[:, :, 0])-np.min(image[:, :, 0])) #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = (image[:, :, 1] - np.min(image[:, :, 1]))/(np.max(image[:, :, 1])-np.min(image[:, :, 1])) #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = (image[:, :, 2] - np.min(image[:, :, 2]))/(np.max(image[:, :, 2])-np.min(image[:, :, 2])) #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        \n",
    "        remaining_batch = len(folder_list) - (num_batches * batch_size)      \n",
    "        batch_data = np.zeros((remaining_batch,len(img_idx),default_y,default_z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "        batch_labels = np.zeros((remaining_batch,5))\n",
    "        for folder in range(remaining_batch): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (num_batches*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    image = cv2.resize(image,(default_y,default_z))\n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = (image[:, :, 0] - np.min(image[:, :, 0]))/(np.max(image[:, :, 0])-np.min(image[:, :, 0]))#normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = (image[:, :, 1] - np.min(image[:, :, 1]))/(np.max(image[:, :, 1])-np.min(image[:, :, 1])) #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = (image[:, :, 2] - np.min(image[:, :, 2]))/(np.max(image[:, :, 2])-np.min(image[:, :, 2])) #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*remaining_batch)].strip().split(';')[2])] = 1\n",
    "        yield batch_data, batch_labels                                                              \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here that a video is represented above in the generator as (number of images, height, width, number of channels). Take this into consideration while creating the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 50\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = 'train'\n",
    "val_path = 'val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 50 # choose the number of epochs\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model building process\n",
    "We will make the model using different functionalities that Keras provides. We have used `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. The last layer is the softmax. We will write the model, the next step is to `compile` the model then we print the `summary` of the model and we will  see the total number of parameters you have to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "#Giving a model name\n",
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "\n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "#creating a checkpoint\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4) # REducelronplateau code\n",
    "#Creating a callbacks\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(Conv3D(8, (2, 2, 2), padding='same',\n",
    "                 input_shape=(len(image_idx), default_z, default_y, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv3D(16, (2, 2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model.add(Dropout(0.05))\n",
    "\n",
    "model.add(Conv3D(16, (2, 2, 2), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv3D(32, (2, 2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model.add(Dropout(0.05))\n",
    "\n",
    "model.add(Conv3D(32, (2, 2, 2), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv3D(32, (2, 2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model.add(Dropout(0.05))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#\n",
    "model.add(Dense(5))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- No. Convolution Layers - 6\n",
    "- Convolution Filter Size- (2,2,2)\n",
    "- Pooling filter size - (2,2,2)\n",
    "- Batch Size - 15\n",
    "- Epochs - 10\n",
    "- Image size - (120, 120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the model and print the summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d (Conv3D)             (None, 30, 120, 120, 8)   200       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 30, 120, 120, 8)   0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 30, 120, 120, 8)  32        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv3d_1 (Conv3D)           (None, 29, 119, 119, 16)  1040      \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 29, 119, 119, 16)  0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 29, 119, 119, 16)  64       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling3d (MaxPooling3D  (None, 14, 59, 59, 16)   0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 14, 59, 59, 16)    0         \n",
      "                                                                 \n",
      " conv3d_2 (Conv3D)           (None, 14, 59, 59, 16)    2064      \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 14, 59, 59, 16)    0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 14, 59, 59, 16)   64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv3d_3 (Conv3D)           (None, 13, 58, 58, 32)    4128      \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 13, 58, 58, 32)    0         \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 13, 58, 58, 32)   128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling3d_1 (MaxPooling  (None, 6, 29, 29, 32)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 6, 29, 29, 32)     0         \n",
      "                                                                 \n",
      " conv3d_4 (Conv3D)           (None, 6, 29, 29, 32)     8224      \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 6, 29, 29, 32)     0         \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 6, 29, 29, 32)    128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv3d_5 (Conv3D)           (None, 5, 28, 28, 32)     8224      \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 5, 28, 28, 32)     0         \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 5, 28, 28, 32)    128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling3d_2 (MaxPooling  (None, 2, 14, 14, 32)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 2, 14, 14, 32)     0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 12544)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               3211520   \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               131584    \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 512)               0         \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 2565      \n",
      "                                                                 \n",
      " activation_8 (Activation)   (None, 5)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,370,093\n",
      "Trainable params: 3,369,821\n",
      "Non-trainable params: 272\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser = tf.keras.optimizers.Adam(learning_rate = 0.001) #optimizer with learning rate =0.001\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  train ; batch size = 15\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - ETA: 0s - loss: 4.1510 - categorical_accuracy: 0.3243 Source path =  val ; batch size = 15\n",
      "\n",
      "Epoch 1: saving model to model_init_2022-05-1722_06_33.726637\\model-00001-4.15101-0.32428-2.87815-0.17000.h5\n",
      "45/45 [==============================] - 1397s 31s/step - loss: 4.1510 - categorical_accuracy: 0.3243 - val_loss: 2.8781 - val_categorical_accuracy: 0.1700 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.6100 - categorical_accuracy: 0.3544 \n",
      "Epoch 2: saving model to model_init_2022-05-1722_06_33.726637\\model-00002-1.61003-0.35445-3.40708-0.14000.h5\n",
      "45/45 [==============================] - 1341s 30s/step - loss: 1.6100 - categorical_accuracy: 0.3544 - val_loss: 3.4071 - val_categorical_accuracy: 0.1400 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.4517 - categorical_accuracy: 0.4344 \n",
      "Epoch 3: saving model to model_init_2022-05-1722_06_33.726637\\model-00003-1.45171-0.43439-5.29215-0.16000.h5\n",
      "45/45 [==============================] - 1324s 29s/step - loss: 1.4517 - categorical_accuracy: 0.4344 - val_loss: 5.2922 - val_categorical_accuracy: 0.1600 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.3714 - categorical_accuracy: 0.4585 \n",
      "Epoch 4: saving model to model_init_2022-05-1722_06_33.726637\\model-00004-1.37141-0.45852-5.08423-0.17000.h5\n",
      "45/45 [==============================] - 1349s 30s/step - loss: 1.3714 - categorical_accuracy: 0.4585 - val_loss: 5.0842 - val_categorical_accuracy: 0.1700 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.2439 - categorical_accuracy: 0.5324 \n",
      "Epoch 5: saving model to model_init_2022-05-1722_06_33.726637\\model-00005-1.24391-0.53243-2.93043-0.19000.h5\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "45/45 [==============================] - 1334s 30s/step - loss: 1.2439 - categorical_accuracy: 0.5324 - val_loss: 2.9304 - val_categorical_accuracy: 0.1900 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.0822 - categorical_accuracy: 0.5505 \n",
      "Epoch 6: saving model to model_init_2022-05-1722_06_33.726637\\model-00006-1.08217-0.55053-2.31894-0.21000.h5\n",
      "45/45 [==============================] - 1321s 29s/step - loss: 1.0822 - categorical_accuracy: 0.5505 - val_loss: 2.3189 - val_categorical_accuracy: 0.2100 - lr: 2.0000e-04\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.0113 - categorical_accuracy: 0.6229 \n",
      "Epoch 7: saving model to model_init_2022-05-1722_06_33.726637\\model-00007-1.01134-0.62293-1.97046-0.31000.h5\n",
      "45/45 [==============================] - 1322s 29s/step - loss: 1.0113 - categorical_accuracy: 0.6229 - val_loss: 1.9705 - val_categorical_accuracy: 0.3100 - lr: 2.0000e-04\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8945 - categorical_accuracy: 0.6621 \n",
      "Epoch 8: saving model to model_init_2022-05-1722_06_33.726637\\model-00008-0.89448-0.66214-1.53378-0.44000.h5\n",
      "45/45 [==============================] - 1360s 30s/step - loss: 0.8945 - categorical_accuracy: 0.6621 - val_loss: 1.5338 - val_categorical_accuracy: 0.4400 - lr: 2.0000e-04\n",
      "Epoch 9/10\n",
      "22/45 [=============>................] - ETA: 8:07 - loss: 0.8794 - categorical_accuracy: 0.6697"
     ]
    }
   ],
   "source": [
    "model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=10, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings\n",
    " - Model is overfitting \n",
    " - Train loss: 0.8216\n",
    " - Train categorical_accuracy: 0.7210 \n",
    " - val_loss: 1.2255 \n",
    " - val_categorical_accuracy: 0.6100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(Conv3D(8, (2, 2, 2), padding='same',\n",
    "                 input_shape=(len(image_idx), default_z, default_y, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv3D(16, (2, 2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Conv3D(16, (2, 2, 2), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv3D(32, (2, 2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Conv3D(32, (2, 2, 2), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv3D(32, (2, 2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model.add(Dense(5))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling and printing the summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = tf.keras.optimizers.Adam(lr = 0.001) #optimizer with learning rate =0.001\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=25, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings\n",
    "- Model is overfitting\n",
    "- Train loss: 0.5430\n",
    "- Train categorical_accuracy: 0.7768\n",
    "- val_loss: 1.1338\n",
    "- val_categorical_accuracy: 0.6900"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(Conv3D(8, (2, 2, 2), padding='same',\n",
    "                 input_shape=(len(image_idx), default_z, default_y, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv3D(16, (2, 2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "#Remove drop out \n",
    "#model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Conv3D(16, (2, 2, 2), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv3D(32, (2, 2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Conv3D(32, (2, 2, 2), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv3D(32, (3, 3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(256, activation = 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model.add(Dense(5))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the model and print the summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = tf.keras.optimizers.Adam(lr = 0.001) # optimizer with learning rate = 0.001\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=25, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings\n",
    "\n",
    "- Overfitting was reduced in the model by removing dropout layer\n",
    "- Train loss: 0.3063\n",
    "- Train categorical_accuracy: 0.9005\n",
    "- val_loss: 0.8488\n",
    "- val_categorical_accuracy: 0.8200"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
